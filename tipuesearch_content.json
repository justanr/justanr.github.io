{"pages":[{"tags":"music","loc":"http://justanr.github.io/demon-hunter-demon-hunter-review","text":"from IPython.display import YouTubeVideo YouTubeVideo ( \"6zFC6tv667c\" ) This was an album that changed the direction of my musical tastes in a very big way. I was fourteen, a pretty straight laced Christian kid. My beard hadn't manifested by then, so I was clean cut and baby faced still. At that point, my musical collection consisted of bands like Five Iron Frenzy and The Aquabats, Reliant K, etc. And the last thing I expected to happen on a church youth retreat was to come back with an interest in metal. There was a talk I attended about expanding horizons or some such. It had to do with christian media and the stereotypes of what christians were allowed to enjoy. The guy giving the talk started talking about seeking out alternatives to bands that we enjoyed. Like KoRn? Try POD . Like Eminem, try KJ -52 (lol). Slipknot, how about Demon Hunter. And on came Infected. Not the whole song, just part of it. My mind was blown . I was like \"wut,\" when I got home, I immediately sought out everything I could find from them. At the time it was just their self-titled and maybe \"Summer of Darkness\" had been released not too long before. I got my hands on the self-titled and I don't think it left my CD player for months. Listening to it now, it all sounds so predictable and derivative. Keep in mind, their big contemporaries were bands like Slipknot, Stone Sour, Disturbed, Drowning Pool and that this album is almost as old as I was when I first heard it. I'll say that it hasn't aged well, in my opinion at least. The songwriting is mediocre. There's nothing really wrong with it, but the album just kinda…sits there. Musically, most of the songs bounce between kind of heavy and mellow hard rock, but don't do anything with it. They toss in a lot of odd flairs sometimes: listen to the chorus of \"Scream of the Undead\" and hear what I mean. It's an acceptable, clean sung chorus but it was this strange squealing guitar half-buried in the background. Ryan Clark's voice is alright. It's not grating but it's not memorable either. He tends to use the intense half-mutter a bit too much for me. However, it's mixed way into the front, it's almost impossible to hear anything over it — I don't know if this is a problem with all the songs I found on youtube or if it was actually like that, but it's surely annoying. Speaking of mixing, I can't tell what's up with the drums. Half the time, they're buried way back in the mix like an after thought but then they'll jump straight forward (with or without making them sound like a variety of banana boxes and plastic storage containers). Overall, it's a very run of the mill early 2000s nu-metal album. It definitely fit with the metal atmosphere of its niche (Christian metaldom) for its time despite being contemporary with Extol's Undeceived, Living Sacrifice's Conceived in Fire and Zao's Parade of Chaos — all of which fare considerably better today, Living Sacrifice is about the only band I haven't revisited outside of nostalgia. Despite all that, I do have fond memories of this album because it was ultimately responsible for my interest in metal, though not the album that made me truly heavy.","title":"Demon Hunter - Demon Hunter Review"},{"tags":"tutorials","loc":"http://justanr.github.io/currying-and-memoizing-metaclasses","text":"I've been playing with the toolz library recently and it's pretty great, especially their implementation of the curry and memoize decorators. However, applying these to classes creates a problem: inheritance. A quick example before we delve into solving this problem: from toolz import curry , memoize @curry class Person ( object ): def __init__ ( self , name , age ): self . name = name self . age = age def __repr__ ( self ): return \"Person(name={!r}, age={!r})\" . format ( self . name , self . age ) p = Person ( name = 'alec' ) p ( age = 26 ) Person(name='alec', age=26) Currying a class we don't expect to inherit from is easy. However, if someone comes along and says, \"I'd like to create a subclass\" then there's an issue… class PersonWithHobby ( Person ): def __init__ ( self , name , age , hobby ): super ( PersonWithHobby , self ) . __init__ ( name , age ) self . hobby = hobby --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-2-12f89c179297> in <module>() ----> 1 class PersonWithHobby(Person): 2 def __init__(self, name, age, hobby): 3 super(PersonWithHobby, self).__init__(name, age) 4 self.hobby = hobby /home/justanr/contrib/toolz/toolz/functoolz.py in __init__(self, func, *args, **kwargs) 155 def __init__(self, func, *args, **kwargs): 156 if not callable(func): --> 157 raise TypeError(\"Input must be callable\") 158 159 # curry- or functools.partial-like object? Unpack and merge arguments TypeError: Input must be callable Oh…that's a problem. Instead, we have to inherit from Person.func in the case of this decorator. Just like if we had partialled the class manually: class PersonWithHobby ( Person . func ): def __init__ ( self , name , age , hobby ): super ( PersonWithHobby , self ) . __init__ ( name , age ) self . hobby = hobby But if you're anything like me that inheritance line is…bothersome. Because we're locking up the original class in the curry decorator, there's no clean way to get it out and just inherit from it other than accessing the decorator's attributes themselves. I tried for about six hours and ended up roping some folks at /r/learnpython into that mess as well. Using the memoize decorator presents the same issue. Instead, what we'd probably like to do is not only inherit from the class, but retain its currying or memoizing characteristics. Metaclasses are to classes as decorators are to functions Ooooh, the scary \"M\" word. Tim Peters once said, Metaclasses are deeper magic than 99% of users should ever worry about. If you wonder whether you need them, you don't (the people who actually need them know with certainty that they need them, and don't need an explanation about why). That's a pretty big warning to attach to something. Metaclasses are deep magic, but it's relatively straight forward magic. If you're unsure about what a metaclass is, check out Eli Bendersky's Python metaclasses by example for an overview. But the short of it is this: Everything in Python is an Object (including: functions, classes and even modules) Classes make objects. If classes are objects, too, it serves to reason that there's a class that makes them That class-making-class is called type. type is our magic that takes a class body and makes it an object. This is the default metaclass for all of our classes. However, we don't want vanilla Python classes. We'd like to have classes that are curryable or can be memoized. The problem with using a decorator is that it'll happily apply to the immediate object, but it generally won't apply to an entire inheritance chain. But a metaclass will. An Aside: new vs a Metaclass Both of what I'm about to show could be achieved by overriding __new__ on a regular class. However, that's no fun. Though, since metaclasses are more about creating classes than instances, I couldn't blame you if you created CurryableMixin and MemoizedMixin classes and called it a day. However, I find cooperative multiple inheritance that uses __new__ to be difficult to manage, especially because object.__new__ accepts no arguments other than the class it makes the object out of. So some sort of sink would be needed to strip off any extra kwargs passed along and then you have to consider if inheriting from an immutable object comes afterwards and if it'll need any of the kwargs and it'll quickly turn into a mess if you try to capture all the corner cases. Curryable Metaclass This is actually the simpler of the two metaclasses to write, surprisingly. Instead of doing any magic in __new__ or __init__ we simply override __call__ instead. __call__ in this case is analogous to __new__ in a regular class, this handles instantiation of an actual object, rather than class creation. from functools import wraps class Curryable ( type ): # one level up from classes # cls here is the actual class we've created already def __call__ ( cls , * args , ** kwargs ): # we'd like to preserve metadata but not migrate # the underlying dictionary @wraps ( cls , updated = []) # distinguish from what was passed to __call__ # and what as passed to currier def currier ( * a , ** k ): return super ( Curryable , cls ) . __call__ ( * a , ** k ) # there's sometimes odd behavior if this isn't done return curry ( currier )( * args , ** kwargs ) class Person ( metaclass = Curryable ): def __init__ ( self , name , age ): self . name = name self . age = age def __repr__ ( self ): return \"Person(name={!r}, age={!r})\" . format ( self . name , self . age ) p = Person ( name = 'alec' ) p ( age = 26 ) Person(name='alec', age=26) curry guards against type errors, allowing us to repeatedly apply arguments until we get something that doesn't throw a TypeError. This also allows us to build inheritance chains where we simply pass up kwargs to the next class: class PersonWithHobby ( Person ): # as an example only; it's still best practice to declare required parameters def __init__ ( self , hobby , ** kwargs ): super ( PersonWithHobby , self ) . __init__ ( ** kwargs ) self . hobby = hobby def __repr__ ( self ): return \"Person(name={!r}, age={!r}, hobby={!r})\" . format ( self . name , self . age , self . hobby ) p = PersonWithHobby ( hobby = 'coding' ) p ( name = 'alec' , age = 26 ) Person(name='alec', age=26, hobby='coding') Memoizing Metaclass This one is quite a bit more difficult to write. Instead of just overriding __call__ , we need to override __init__ as well and provide a key-value store. Instead of throwing a bunch of code all at once, I'd rather disect it bit by bit: default_cache_key By default, memoize will attempt to do the right thing. However, it uses the inspect module to determine if there's keyword arguments. This can act oddly sometimes and if memoize doesn't detect keyword arguments, it'll only memoize positional arguments. Instead, we'd like to always memoize both. We could go further and attempt to bind positional keyword arguments with their actual names, but for now, this will suffice. def default_cache_key ( args , kwargs ): return ( args or None , frozenset ( kwargs . items ()) or None ) HybridValueStore If you're not familiar with descriptors, I recommend Chris Beaumont's Python Descriptors Demystified and Simeon Franklin's Descriptor talk We'll also have to centralize our cache so we can control it. However, this presents a problem. If we have two memoized classes, they shouldn't be able to poke at each other's caches. So simply setting a dictionary on the metaclass won't work. Rather we need to allow each class to only access it's particular cache and actual instances of the class probably shouldn't have access to the cache directly either since their only business with it is existing in it. And overriding a class's cache should also affect the master cache as well so the two remain consistent. And deleting a class's cache simply pops it from the master cache. With that in mind, we can write a descriptor that wraps any key-value store and either return the whole store if it's the metaclass accessing it or, if it's a memoized class accessing it, the descriptor will return just the class's cache. Since we're one level up from classes and instances, I've commented which parameters correspond to the class and metaclass. class HybridValueStore ( object ): def __init__ ( self , valuestore ): self . valuestore = valuestore # |+------------------> The Descriptor Instance # | |+------------> The Memoized Class # | | |+------> The Metaclass def __get__ ( self , inst , cls ): if inst is None : return self . valuestore else : return self . valuestore [ inst ] def __set__ ( self , inst , value ): self . valuestore [ inst ] = value def __delete__ ( self , inst ): self . valuestore . pop ( inst , None ) Actual Metaclass Now, with those two out of the way, we can actually put the pieces together. from toolz import memoize class Memoized ( type ): cache = HybridValueStore ({}) cache_key = HybridValueStore ({}) def __new__ ( mcls , name , bases , attrs , ** kwargs ): return super ( Memoized , mcls ) . __new__ ( mcls , name , bases , attrs ) def __init__ ( cls , name , bases , attrs , key = default_cache_key , cache = None ): if cache is None : cache = {} cls . cache = cache cls . cache_key = key super ( Memoized , cls ) . __init__ ( name , bases , attrs ) def __call__ ( cls , * args , ** kwargs ): @memoize ( cache = cls . cache , key = cls . cache_key ) def memoizer ( * a , ** k ): return super ( Memoized , cls ) . __call__ ( * a , ** k ) return memoizer ( * args , ** kwargs ) The master cache is implemented with HybridValueStore using a regular dictionary that we add further mappings. Since we've provided a __set__ method, we can use a normal dictionary rather than something like defaultdict which provides just-in-time access to keys. We also use the same thing with the cache_keys as well. Originally, I had planned on storing the key on the class's cache, but seeing as dict can't host arbitrary attributes, that plan fell through. Rather, storing the key alongside the cache as a seperate attribute seems to function just fine. __new__ is where things start to get strange. In addition to the normal parameters it accepts, there's also **kwargs . This is to allow passing keyword arguments to the metaclass, which we'll see in a moment. In __init__ is where the extra keywords come into play: key is the function we'll use to create cache keys and defaults to the function described above, cache is the mapping for storing instances. If it's not provided, it simply defaults to a regular dictionary. However, this allows using things like weakref.WeakValueDictionary or another specialized mapping as the container rather than a regular dictionary. Both of these are simple stored on the instance of the metaclass (which is the created class) but, interestingly, these aren't available to the instances created from the class. And finally, __call__ is where the memoization actually happens. A wrapper is created to memoize and provided with the class's cache and cache key function and the actual object instantiation is delegated to the next metaclass in the MRO (typically type ). In Action After all that, let's see this bad boy do its work, just two simple classes will work. class Frob ( metaclass = Memoized ): def __init__ ( self , frob ): self . frob = frob def __repr__ ( self ): return \"Frob({})\" . format ( self . frob ) # simply here to show HybridValueStore's fine grained access class Dummy ( metaclass = Memoized ): def __init__ ( self , * args , ** kwargs ): pass def __repr__ ( self ): return \"Dummy\" f = Frob ( 1 ) d = Dummy () assert f is Frob ( 1 ), \"guess it didn't work\" That went well. Let's see some other parts in action: print ( \"Master Cache: \" , Memoized . cache ) print ( \"Frob Cache: \" , Frob . cache ) print ( \"Dummy Cache: \" , Dummy . cache ) Master Cache: {<class '__main__.Dummy'>: {(None, None): Dummy}, <class '__main__.Frob'>: {((1,), None): Frob(1)}} Frob Cache: {((1,), None): Frob(1)} Dummy Cache: {(None, None): Dummy} Good to see the fine-grained access to the cache attribute is working. How about if we reset the cache for Frob? Frob . cache = {} print ( \"Master Cache: \" , Memoized . cache ) print ( \"Frob Cache: \" , Frob . cache ) print ( \"Dummy Cache: \" , Dummy . cache ) Master Cache: {<class '__main__.Dummy'>: {(None, None): Dummy}, <class '__main__.Frob'>: {}} Frob Cache: {} Dummy Cache: {(None, None): Dummy} Awesome. Now, there was the curious keyword arguments that we can pass to the metaclass…but how ? It's simple, we pass them the same way metaclasses are declared (at least in Python 3): from collections import OrderedDict def make_string_key ( args , kwargs ): return str ( args ) + str ( kwargs ) class KeywordTest ( metaclass = Memoized , key = make_string_key , cache = OrderedDict ()): def __init__ ( self , * args , ** kwargs ): pass kwt1 = KeywordTest ( 1 , 2 , 3 ) kwt2 = KeywordTest ( 4 , 5 , 6 ) print ( KeywordTest . cache ) OrderedDict([('(1, 2, 3){}', <__main__.KeywordTest object at 0x7fcdb018f358>), ('(4, 5, 6){}', <__main__.KeywordTest object at 0x7fcdb018f908>)]) Now we have a cache that keeps order of when it's values were created. Something curious about this setup is that instances of the memoized class can't access the cache. f . cache --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-18-e5f688beb613> in <module>() ----> 1 f.cache AttributeError: 'Frob' object has no attribute 'cache' Which is very handy considering we probably don't want instances accidentally mucking about and overwriting the cache. Currying AND Memoization Metaclass What if we wanted currying and memoization on the same class? Seems impossible since Python imposes a restriction of one metaclass per inheritance chain. However, since metaclasses are just regular classes, we can compose them together to form much more complex metaclasses. Notice how I was using super to call to things like __new__ , __init__ and __call__ above rather than explicitly saying, type.__new__ ? This was to allow for this exact thing. With that already in place, all we need to do to create a curried and memoized class is to just place those two metaclasses together. However, there is one thing that needs to be noted: order matters. See Raymond Hettinger's PyCon 2015 talk Super Considered Super to see why. If we want to curry then memoize, we simply do this: class CurriedMemoized ( Curryable , Memoized ): pass class CMTester ( metaclass = CurriedMemoized ): def __init__ ( self , * args , ** kwargs ): pass So far so good. Let's test it out… CMTester ( 1 , 2 , 3 ) print ( CMTester . cache ) {((1, 2, 3), None): <__main__.CMTester object at 0x7fcdb011c438>} What about taking advantage of Memoized keyword arguments? class CMKeywordTest ( metaclass = CurriedMemoized , key = make_string_key , cache = OrderedDict ()): def __init__ ( self , * args , ** kwargs ): pass CMKeywordTest ( 1 , 2 , 3 ) CMKeywordTest ( 4 , 5 , 6 ) print ( CMKeywordTest . cache ) OrderedDict([('(1, 2, 3){}', <__main__.CMKeywordTest object at 0x7fcdb018fdd8>), ('(4, 5, 6){}', <__main__.CMKeywordTest object at 0x7fcdb018fd68>)]) Now, if we had swapped Memoized and Curryable around in the MRO , we'd get compeletly different behavior: class MemoizedCurry ( Memoized , Curryable ): pass class MCTest ( metaclass = MemoizedCurry ): def __init__ ( self , name , frob ): pass m = MCTest ( name = 'default frob' ) m ( frob = 1 ) print ( MCTest . cache ) {(None, frozenset({('name', 'default frob')})): <function MCTest at 0x7fcdb01836a8>} In this case, we're memoizing just what's partially applied rather than the actual instance. In this particular case, it's probably undesired behavior, but with other metaclasses, this might be the intended order of operations. Parting Thoughts Hopefully this has been a nice introduction to metaclasses and has shown some pratical applications of them rather than some silly examples. If you're still curious about writing your own metaclasses or learning more about them, here's some resources I recommend: Mike Mueller - Descriptors and Metaclasses Dave Beazley - Python 3 Metaprogramming Craig de Stigter - Intro to Metaclasses","title":"Currying and Memoizing Metaclasses"},{"tags":"tutorials","loc":"http://justanr.github.io/quotly-building-a-simple-json-api-with-flask-marshmallow-and-sqlalchemy","text":"My love of Flask is well known. It's a great microframework that puts you in control of the pieces needed in your webapp. It comes with templating, requests, responses, cookies, sessions, routing and… really, that's it. Routes can be built by decorating functions or by inheriting from flask.views.View or flask.views.MethodView depending on your needs. Something I've explored in the past is building a simple RESTful service using Flask and SQLAlchemy. However, that blog post fell into the trap of \"Look at all this code!\" that happens to me regretfully often. D: However, I'd like revisit the idea and explore the topic a little more in depth. I will forewarn, that this post is still pretty code heavy. If that's the sort of thing that makes your eyes glaze over, you may want to go to /r/pitbulls instead. Quotly: Cheeky Movie Quotes for Web 2.0 Okay, lame. But it's not another TODO list, though not much better. But by the end, hopefully we'll feel comfortable with Flask, Flask-SQLAlchemy, Marshmallow (12/10 serializer library) and a little bit of Flask-Restful (though, it's going to be used as a base that we're going to modify). This post was written with Python 3.4, but it should work with at least 2.7 and 3.3. I'm not sure about others and I'm too lazy to add the deadsnakes PPA right now. D: You can follow along with vim or PyCharm or whatever. The full application is at the bottom of the post as well if you'd rather peek now. If you are following along, I recommend making two files: app.py and run.py . app.py is where we'll obviously be building Quotly. The run.py will simply run the debug server on the side, just set that one up to look like this: from app import app if __name__ == '__main__' : app . run ( debug = True , reload = True ) Step Zero: Needed packages Instead of installing packages along every step, let's just get all that mess out the way now… pip install --user -U marshmallow --pre pip install --user -U flask-sqlalchemy flask-restful That'll install everything you need to follow along here. A note about that Marshmallow install: This installs a pre-release version of Marshmallow, which we'll need to take advantage of some cool stuff that's coming in Marshmallow. If you're also wanting to use Flask-Marshmallow (perhaps to avoid writing your own hyperlink serializer), install it afterwards to avoid getting a potentially older version. Step One: Considering Our Data First, let's consider what our data looks like. We have quotes. People say quotes. And that's really about it. We could use a list of dictionaries for this, but since we'll eventually involve SQLAlchemy which returns objects, lets use namedtuple instead. from collections import namedtuple # basic quote object # can also use to represent many with [Quote(person..., quote=...)[,...]] Quote = namedtuple ( 'Quote' , [ 'person' , 'quote' ]) # Let's prepopulate an example quote list quotes = [ Quote ( p , q ) for p , q in [ ( \"Herbert West\" , \"I was busy pushing bodies around as you well know \" \"and what would a note say, Dan? 'Cat dead, details later'?\" ), ( \"Jack Burton\" , \"You remember what ol' Jack Burton always says at a time like that: \" \"'Have ya paid your dues, Jack?' 'Yessir, the check is in the mail.'\" ), ( \"Igor\" , \"Well, why isn't it Froaderick Fronkensteen?\" ) ]] I wouldn't blame you if you took a break to track down one of these movies and turned it on the background. from IPython.display import Image Image ( url = \"http://i.imgur.com/9VeIbMZ.gif\" ) Step Two: Serializing to JSON Assumuing you've not been distracted, let's see about taking these quote objects and turning them into JSON . import json print ( json . dumps ( quotes [ 0 ], indent = 2 )) [ \"Herbert West\", \"I was busy pushing bodies around as you well know and what would a note say, Dan? 'Cat dead, details later'?\" ] …um…er…That's not what we really wanted. Since JSON has no notation of tuples, let alone namedtuples, Python helpfully transforms them into JSON 's nearest relation: lists. However, we'd probably find it nicer to have key-value pairs pop out the otherside. Of course, we could just use a dictionary, or write a namedtuple_to_dict function that'll do it ourselves: namedtuple_to_dict = vars print ( json . dumps ( namedtuple_to_dict ( quotes [ 0 ]), indent = 2 )) { \"person\": \"Herbert West\", \"quote\": \"I was busy pushing bodies around as you well know and what would a note say, Dan? 'Cat dead, details later'?\" } But that's no fun and will only work one level deep. What happens when we need to serialize objects that have other objects living inside them? That won't work. I've seen lots of ways to handle this, most of them are just variations on a __json__ method on every object and subclassing json.JSONEncoder to just invoke that when it encounters something it can't serialize. Plus, it still wouldn't work for namedtuple since it can be serialized to a list. Image ( url = \"http://i.imgur.com/mWU6lP6.gif\" ) Rather than hacking some function or a mixin together and making the object responsible for knowing how to transform itself into a dictionary, why not use a robust, well tested object serializer library? No, not pickle — pickles are unsafe and too vinegary for me. My sweet tooth is craving Marshmallows. from marshmallow import Schema , pprint from marshmallow.fields import String class QuoteSchema ( Schema ): person = String () quote = String () pprint ( QuoteSchema () . dump ( quotes [ 0 ]) . data ) {'person': 'Herbert West', 'quote': 'I was busy pushing bodies around as you well know and what would ' \"a note say, Dan? 'Cat dead, details later'?\"} …wait, is really that easy? Five lines, including the imports? It seems like it shouldn't be, but it is. Actually it can even be easier: class QuoteSchema ( Schema ): class Meta : additional = ( 'person' , 'quote' ) pprint ( QuoteSchema () . dump ( quotes [ 1 ]) . data ) {'person': 'Jack Burton', 'quote': \"You remember what ol' Jack Burton always says at a time like \" \"that: 'Have ya paid your dues, Jack?' 'Yessir, the check is in \" \"the mail.'\"} Marshmallow is smart enough to know how to serialize built-in types without us saying, \"This is a string.\" Which is fantastic. We can take that schema and json.dumps and produce what we actually wanted: print ( json . dumps ( QuoteSchema () . dump ( quotes [ 2 ]) . data , indent = 2 )) { \"quote\": \"Well, why isn't it Froaderick Fronkensteen?\", \"person\": \"Igor\" } And unlike many other solutions, Marshmallow will also allow us to serialize a collection of objects as well: pprint ( QuoteSchema ( many = True ) . dump ( quotes ) . data ) [{'person': 'Herbert West', 'quote': 'I was busy pushing bodies around as you well know and what ' \"would a note say, Dan? 'Cat dead, details later'?\"}, {'person': 'Jack Burton', 'quote': \"You remember what ol' Jack Burton always says at a time like \" \"that: 'Have ya paid your dues, Jack?' 'Yessir, the check is in \" \"the mail.'\"}, {'person': 'Igor', 'quote': \"Well, why isn't it Froaderick Fronkensteen?\"}] While this is valid JSON (a root object can be either an object or an array), Flask will only allow objects at the root level to prevent stuff like this . However, asking a schema to create a dictionary if it serializes a collection isn't hard to do at all: from marshmallow import post_dump class QuoteSchema ( Schema ): class Meta : additional = ( 'person' , 'quote' ) @post_dump ( raw = True ) def wrap_if_many ( self , data , many = False ): if many : return { 'quotes' : data } return data pprint ( QuoteSchema ( many = True ) . dump ( quotes ) . data ) {'quotes': [{'person': 'Herbert West', 'quote': 'I was busy pushing bodies around as you well know ' \"and what would a note say, Dan? 'Cat dead, details \" \"later'?\"}, {'person': 'Jack Burton', 'quote': \"You remember what ol' Jack Burton always says at a \" \"time like that: 'Have ya paid your dues, Jack?' \" \"'Yessir, the check is in the mail.'\"}, {'person': 'Igor', 'quote': \"Well, why isn't it Froaderick Fronkensteen?\"}]} Image ( url = \"http://i.imgur.com/BUtt2Jd.gif\" ) Step Three: Briefly Flask Now that the Quote objects can be correctly serialized to JSON , feeding it from Flask is easy peasy. from flask import Flask , jsonify app = Flask ( 'notebooks' ) # reuse the same QuoteSchema instance rather than creating new with each request QuoteSerializer = QuoteSchema () @app.route ( '/quote/<int:id>' ) def single_quote ( idx ): if not 0 <= idx < len ( quotes ): # flask allows return a tuple of data, status code, headers (dict) # status code is 200 by default data = { 'error' : 'quote out of range' }, 400 else : data = QuoteSerializer . dump ( quote [ idx ]) . data return data Step Four: Deserialization However, only getting a quote is pretty simple stuff. What if we wanted to create new Quote objects from JSON ? This is pretty easy to do by hand with Flask's request object (note: the request.get_json method is currently the recommended method for plucking JSON out of the request rather than using the request.json attribute): from flask import request @app.route ( '/quote/' , methods = [ 'POST' ]) def make_new_quote (): # get_json returns dict or None on failure json = request . get_json () if json and 'quote' in json : quotes . append ( Quote ( person = json [ 'person' ], quote = json [ 'quote' ])) msg = { 'success' : True , 'msg' : 'Added quote.' } else : msg = { 'success' : False , 'msg' : 'must specify quote in JSON request' }, 400 return msg However, if we're deserializing complex objects, say a tracklist that has an attribute that holds track objects which reference artist objects. Pretty soon manually deserializing an object becomes quite…messy. However, there is a better way. Marshmallow not only serializes objects, but will also handle deserialization if we give it a little bit of help: class QuoteSchema ( Schema ): class Meta : additional = ( 'person' , 'quote' ) @post_dump ( raw = True ) def wrap_if_many ( self , data , many = False ): if many : return { 'quotes' : data } return data def make_object ( self , data ): assert 'person' in data and 'quote' in data , \"Must specify person and quote in request\" return Quote ( person = data [ 'person' ], quote = data [ 'quote' ]) QuoteSchema () . load ({ \"person\" : \"Ash\" , \"quote\" : \"Good. Bad. I'm the guy with the gun.\" }) . data Quote(person='Ash', quote=\"Good. Bad. I'm the guy with the gun.\") Just the opposite of what we had before. Dictionary in, Object out. We can also deserialize a collection as well: QuoteSchema ( many = True ) . load ([ { 'person' : 'Ash' , 'quote' : \"Good. Bad. I'm the guy with the gun.\" }, { 'person' : 'Shaun' , 'quote' : \"You've got red on you.\" } ]) . data [Quote(person='Ash', quote=\"Good. Bad. I'm the guy with the gun.\"), Quote(person='Shaun', quote=\"You've got red on you.\")] Hopefully the advantage of using Marshmallow for even sending and receiving simple JSON objects is apparent. With 11 lines we can take an object and cast it to a dictionary and we can take a dictionary with certain keys and build an object with it. Sure, we're just serializing and deserializing a namedtuple…\"But that's how it always begins. Very small.\" Image ( url = \"http://i.imgur.com/zvv3ymL.gif\" ) Step Five: Routing with Flask-Restful Flask-Restful is a great library that builds on top of Flask's MethodView and makes it pretty easy to support multiple API styles ( XML , CSV , JSON , etc). It ships with JSON serialization by default, leaving the others up to the user to implement. There's a bunch of other features as well, but I'm only to tread on the incredibly useful routing mechanism in place here. All we need to do to hook into this is to inherit from flask_restful.Resource and return dictionaries from our methods. Dictionaries like the ones produced by Marshmallow. Changing the routing from vanilla Flask routing to class based is a little weird at first, but it quickly becomes very intuitive. And, since the methods for deserialization are in place, let's also handle accepting JSON and appending quotes to our little list. from flask.ext.restful import Resource from flask import request class SingleQuote ( Resource ): def get ( self , idx ): if idx and 0 <= idx < len ( quotes ): # flask-restful also allows data, status code, header tuples return QuoteSerializer . dump ( quotes [ idx ]) . data return { 'error' : 'quote index out of range' }, 400 class ListQuote ( Resource ): def get ( self ): return QuoteSerializer . dump ( quotes , many = True ) . data def post ( self ): json = request . get_json () if not json : return { \"success\" : False , \"msg\" : \"malformed request\" }, 400 if not 'quote' in json : return { \"success\" : False , \"msg\" : \"must specify quote in request\" }, 400 else : # remember QuoteSchema.make_object causes an assert try : q = QuoteSerializer . load ( request [ 'json' ]) . data except AssertionError as e : return { 'success' : False , 'msg' : str ( e )}, 400 else : quotes . append ( q ) return { \"success\" : True , \"msg\" : \"Quote added.\" } And then we simply register these resources on an API object that's hooked up to our application: from flask.ext.restful import Api api = Api ( app ) api . register_resource ( SingleQuote , '/quote/<int:id>' ) api . register_resource ( ListQuote , '/quotes' ) Step Six: Persistence with SQLA This is great and all, but these quotes will only last as long as the session runs. If we need to restart, we lose it all except for the preloaded quotes. To achieve real persistence, we should shake up with a database. SQLite is a good choice for this, plus bindings come native with Python. from flask.ext.sqlalchemy import SQLAlchemy app . config [ 'SQLALCHEMY_DATABASE_URI' ] = 'sqlite:///quotes.db' db = SQLAlchemy ( app ) class Quote ( db . Model ): id = db . Column ( db . Integer , primary_key = True ) person = db . Column ( db . Unicode ( 50 )) quote = db . Column ( db . UnicodeText ) Our schema doesn't change at all. Marshmallow doesn't know or care if we're passing a namedtuple or a SQLA model, just that it has the correct attributes. This is great because we can write many quick tests with something like namedtuple to verify our schema behaves correctly and then just a few integration tests with the models. Image ( url = \"http://i.imgur.com/mudwVxd.gif\" ) However, our resource end points do need to change some, though. Since we're dealing with SQLA models now and not just simple lists. The changes are trivial: class SingleQuote ( Resource ): def get ( self , idx ): if idx : quote = Quote . query . get ( idx ) return QuoteSerializer . dump ( quote ) . data if quote else { \"error\" : \"quote does not exist\" } return { 'error' : 'must specify quote id' } class ListQuote ( Resource ): def get ( self ): return QuoteSerializer . dump ( quotes , many = True ) . data def post ( self ): json = request . get_json () if not json : # get_json will return a dict or None return { \"success\" : False , \"msg\" : \"malformed request\" }, 400 if not 'quote' in json : return { \"success\" : False , \"msg\" : \"must specify quote in request\" }, 400 else : try : q = QuoteSerializer . load ( request [ 'json' ]) . data except AssertionError as e : return { 'success' : False , 'msg' : str ( e )}, 400 else : db . session . add ( q ) db . session . commit () return { \"success\" : True , \"msg\" : \"Quote added.\" } Just two simple changes to go from list to SQLA models. Be sure to run db.create_all() somewhere before and load up initial quotes and Quotly is up and running, ready to send and receive cheeky movie quotes for everyone. Parting Thoughts While this was more of a \"hit the ground running\" guide to building a simple REST API with Flask and its little ecosystem, I hope it's been enlightening. I've included the whole application in this gist for reference. If you see a bug or have questions, hit me up on twitter or on github .","title":"Quotly: Building a simple JSON API with Flask, Marshmallow and SQLAlchemy"},{"tags":"tutorials","loc":"http://justanr.github.io/i-wrote-a-monad-tutorial-for-some-reason","text":"I swore to myself up and down that I wouldn't write one of these. But then I went and hacked up Pynads. And then I wrote a post on Pynads. And then I posted explainations about Monads on reddit. So what the hell. I already fulfilled my \"Write about decorators when I understand them\" obligation and ditto for descriptors. So Monads, why not… It's simple, a monad is like a… No. Stooooooop. :( Burritos. Bucket brigades. Semicolons. All these analogies just confused me for a long time. And then I \"got them\" and by \"got them\" I mean \"Even more hopelessly confused but I didn't know that.\" Like what does \"programmable semicolon\" even mean? Every language I've used (which isn't many) a semicolon means \"This bit of code ends here, kthxbai\". The burrito analogy was meant as a critique of this phenomenon — and I'll likely fall victim of the \"Monad Tutorial Curse\". And the bucket brigade was a valiant effort by a SO user to explain them. It's simple, a monad is like a Unix Pipe Instead of reaching for some non-programming analogy like burritos or bucket brigades, I think Unix Pipes are a pretty good analogy to Haskell-style monads. Let's say I'm in a directory that has a bunch of different types of files — maybe it's the bottomless bin that is ~/Downloads ): And I want to find all the MP4 files in the top level directory and print them out: ls -lh ~/Downloads | grep -i \"*mp4\" | less Super simple. We take the first command ls feed it some options and a directory to list out. Then | goes \"Oh, you have output and I have this thing that needs input, here grep!\" And then grep does its business and | steps back in and goes \"Oh, you have output and I have this thing that needs input, here less!\" Of course it isn't a perfect analogy. But all analogies break down under scrutiny. But this is essentially what Haskell's >>= does. \"Oh, you have output, let me feed it to this function that wants input!\" That's it. Monads are about chaining together a series of actions of functions (depending on how you want to look at it) in a way that each action/function returns something that can carry the chain forward somehow. But the short of monads is that they have nothing to do with I/O, impure values, side effects or anything else. Those are implementation specific to certain monads. Monads in general only deal with how to combine expressions. But Python doesn't have monads Eh. It all depends on how you want to look at it. Sure, it doesn't have Haskell style monads. But it doesn't need to. Let's look at something: x = y = ' Fred \\n Thompson ' I have that input. But I need output that looks like this: \" JACK THOMPSON \" . The obvious way is doing it imperatively: x = x . replace ( 'Fred' , 'Jack' ) x = x . replace ( ' \\n ' , '' ) x = x . strip () x = x . upper () print ( x ) JACK THOMPSON And it works. Or I could just chain all those operations together: print ( y . replace ( 'Fred' , 'Jack' ) . replace ( ' \\n ' , '' ) . strip () . upper ()) JACK THOMPSON Each string method returns a new string that can carry the chain forward. We can add in as many string methods that return a string. But if we place something like split or find then our chain can't be continued as there's a list or a integer now. That's not to say we can't continue the chain, but we likely need to do in a separate expression (which is okay). Worshipping at the altar of bind So Haskell style monads are pretty much defined by the presence of >>= and return . return just lifts a value into a monad. And >>= is the sequencing operator. Neither of these are magic, we need to define them ourselves. I like using Maybe as an example because it's simple enough to explain but addresses a real world problem: Null Pointer Exceptions. (: We usually avoid this sort of thing with this pattern in Python: def sqrt ( x ): if x is None : return None return x **. 5 print ( sqrt ( 4 )) print ( sqrt ( None )) 2.0 None We can use this to process information from STDIN (for example): def int_from_stdin (): x = input () return int ( x ) if x . isdigit () else None maybe_int = int_from_stdin () print ( sqrt ( maybe_int )) a None maybe_int = int_from_stdin () print ( sqrt ( maybe_int )) 4 2.0 We just have to make sure we include the if x is None check everywhere. That's easy. Right. …right? guise? On top of it being something to remember, it's line noise. Completely in the way of what we're attempting to accomplish. Instead, let's look at Maybe in terms of Haskell and Python: data Maybe a = Nothing | Just a instance Monad Maybe where return = Just (Just x) >>= f = f x Nothing >>= f = Nothing We have the type constructor Maybe which has two data constructors Just and Nothing. In Python terms, we have an abstract class Maybe and two implementations Just and Nothing. When we have a Just and >>= is used, we get the result of the function with the input of whatever is in Just. If we have Nothing and >>= is used, we get Nothing ( *Nothing from nothing leaves nothing. You gotta have something, if you wanna be with me* ). Notice that onus to return a Maybe is on whatever function we bind to. This puts the power in our hands to decide if we have a failure at any given point in the operation. In Python, a simplified version looks a lot like this: class Maybe : @staticmethod def unit ( v ): return Just ( v ) def bind ( self , bindee ): raise NotImplementedError class Just ( Maybe ): def __init__ ( self , v ): self . v = v def __repr__ ( self ): return 'Just {!r}' . format ( self . v ) def bind ( self , bindee ): return bindee ( self . v ) class Nothing ( Maybe ): def bind ( self , bindee ): return self def __repr__ ( self ): return 'Nothing' And we can use this to reimplement our int_from_stdin and sqrt functions above: def int_from_stdin (): x = input () return Just ( int ( x )) if x . isdigit () else Nothing () def sqrt ( x ): return Just ( x **. 5 ) And chain them together like this: int_from_stdin () . bind ( sqrt ) 4 Just 2.0 int_from_stdin () . bind ( sqrt ) a Nothing What >>= does isn't just sequence actions together. That's easy to do, we could have accomplished them the same thing before with sqrt(int_from_stdin()) . However, the real magic sauce of >>= is abstracting how they're sequenced. In this case, sequencing a Just results in feeding the contained value of Just to a function and getting back a Maybe. And sequencing a Nothing results in Nothing. The great thing about Maybe is we're allowed to decide at an arbitrary point if we even want to continue with the computation or bail out completely. Let's say we have something against even numbers. Perhaps it's that only one of them is Prime. But we like odds. So if we get an even number from STDIN , we'll just bail out. def only_odds ( x ): return Just ( x ) if x & 1 else Nothing () int_from_stdin () . bind ( only_odds ) . bind ( sqrt ) 4 Nothing int_from_stdin () . bind ( only_odds ) . bind ( sqrt ) 3 Just 1.7320508075688772 Other ways to sequence Obviously bind/ >>= isn't the only way to interact with monads if they're just about sequencing functions together. For example, Scala has a suped-up version of Maybe called Option. It's the same basic structure: Some (our successful computation) and None (a failed computation). It also has ways of recovering from a possibly failed computation with its getOrX methods. For example, if we have Some(\"abc\") we can do this to recover when check if d is present: Some(\"abc\") filter (i => match i indexOf \"d\" { case -1 => None case _ => Some(i) } }) getOr \"d\" Which should return \"d\" but Scala isn't my mother tongue, so there's probably an error somewhere. You could argue that SQLAlchemy is monadic as well based on how you build queries in it: q = session.query(Person).filter(Person.name.startswith('A')).first() SQLAlchemy queries return query objects that can carry the chain further, allowing us to craft complicated queries in a relatively simple manner. I found a more clever example in a thread on /r/learnpython about what features would you implement in Python given that chance. Below the \"Everything not nailed down in Haskell\" comment, there was one about universal function call syntax from D. /u/AMorpork proposed simply creating a monad where __getattr__ is the sequencing operation (reproduced here): from itertools import islice import builtins as __builtin__ def take ( n , it ): return islice ( it , n ) class UFCS ( object ): def __init__ ( self , value ): self . state = value def __getattr__ ( self , item ): try : func = getattr ( __builtin__ , item ) except AttributeError : func = globals ()[ item ] def curried ( * args ): if not args : self . state = func ( self . state ) else : args = list ( args ) args . append ( self . state ) self . state = func ( * args ) return self return curried def get ( self ): return self . state x = [ '#3.462289264065068' , '4.283990003510465' , '#1.7285949138067824' , '#2.6009019446392987' , '5.089491698891653' , '3.854140130424576' , '4.118846086899804' , '5.110436429053362' , '9.044631493138326' , '5.503343391187907' , '1.4415742971795897' , '2.7162342709197618' , '9.438995804377226' , '1.8698624486908322' , '4.008599242523804' , '8.914062382096017' , '4.120213633898632' , '6.9189185117106975' , # more were included, but removed here ] UFCS ( x ) . filter ( lambda s : s and s [ 0 ] != \"#\" ) . map ( float ) . sorted () . take ( 10 ) . list () . print () [1.4415742971795897, 1.8698624486908322, 2.7162342709197618, 3.854140130424576, 4.008599242523804, 4.118846086899804, 4.120213633898632, 4.283990003510465, 5.089491698891653, 5.110436429053362] <__main__.UFCS at 0x7fd4c064ee10> It's simple, a monad is like a… Hopefully this goes a long way to explaining the idea of Monads in terms of programming. Maybe I fell upon the Monad Tutorial Fallacy. However, in the event that I've hopeless confused someone more, drop me a line and I'll be happy to go into further detail.","title":"I wrote a monad tutorial for some reason…"},{"tags":"rants","loc":"http://justanr.github.io/pep-484-and-me","text":"So PEP 484 is a thing. It's about type hinting in Python and seems to be heavily influenced by mypy-lang. However, this isn't a type system . It's meant as a helper for static code analysis. There's no type enforcement — at least to my understanding. Basically, we'd be able to load up pyflakes or PyCharm and receive information on what the parameters are expected to be or if at some point we've got a type mismatch. There's been a lot of talk about this. Some in favor, some not. On one hand, I get it. This is super helpful for analysing a new code base — assuming it's been used. :/ On the other hand, it's down right ugly . I'm not a big fan of inlining types, at all. Some things aren't so bad… import typing as t def add ( x : int , y : int ) -> int : return x + y Not so bad. Just a simple add function, we see it takes two ints and returns an int. However, for something more complicated, let's say zipWith it's gets ugly really fast. Here's the comparable Haskell type: zipWith (a -> b -> c) -> [a] -> [b] -> [c] And here's the proposed PEP syntax: A , B , C = t . TypeVar ( 'A' ), t . TypeVar ( 'B' ), t . TypeVar ( 'C' ) def zip_with ( func : t . Callable [[ A , B ], C ], a : t . List [ A ], b : t . List [ B ]) -> t . List [ C ]: return map ( func , a , b ) There's so much information in the parameter line I can hardly see what's actually relavant. This is something that really bothers me about all inlined types. Here's the proposed PEP syntax for something as simple as compose: # compose :: (b -> c) -> (a -> b) -> (a -> c) def compose ( f : t . Callable [[ B ], C ], g : t . Callable [[ A ], B ]) -> t . Callable [[ A ], C ]: return lambda x : f ( g ( x )) print ( compose . __annotations__ ) {'f': typing.CallableCallable[[~B], ~C], 'return': typing.CallableCallable[[~A], ~C], 'g': typing.CallableCallable[[~A], ~B]} Using a decorator was explictly shot down in the PEP under the argument that it's verbose and function parameters would need to be repeated. However, I find the current proposed syntax to already be verbose. Moreover, a special type of file was proposed: Stub files. These would be additional files maintainers right that mirror the structure of an existing project only to provide annotated functions. If decorators are being shot down as unnecessarily verbose, this should too even if addresses the issue of Python 2 and 3 compatibility. I surely don't want to maintain essentially two copies of my project structure to get the minimal benefits of type hinting. And I certainly think that projects that begin using these will see a decline in contributitions — if your project is using stub files already, surely the onus will be on the committer to maintain changes in the stubs as well. Breaking out the type definitions into a separate line would go a long way to clean it up. Retyping parameters shouldn't be needed, just doing something like this would help: @typed(t.Callable[[B], C], t.Callable[[A], B], returns=t.Callable[[A], C]) def compose(f, g): return lambda x: f(g(x)) Using the special keyword syntax introduced in Python 3.0 provides a clean break between input and output types. And using a decorator to separate the concern of \"this is type information\" from \"these are the parameters\" is what decorators do. As a proof of concept: import inspect from functools import wraps def typed ( * types , returns ): def deco ( f ): # todo handle *args, **kwargs params = inspect . getargspec ( f ) . args if not len ( types ) == len ( params ): raise TypeError ( \"Must provide types for all parameters\" ) annotations = { a : t for a , t in zip ( params , types )} annotations [ 'return' ] = returns f . __annotations__ = annotations @wraps ( f ) def wrapper ( * args , ** kwargs ): return f ( * args , ** kwargs ) return wrapper return deco @typed ( t . Callable [[ B ], C ], t . Callable [[ A ], B ], returns = t . Callable [[ A ], C ]) def compose ( f , g ): return lambda x : f ( g ( x )) compose . __annotations__ {'f': typing.CallableCallable[[~B], ~C], 'g': typing.CallableCallable[[~A], ~B], 'return': typing.CallableCallable[[~A], ~C]} @typed ( A , returns = C ) def mismatched ( a , b ): pass --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-7-2d7bfefaa7d3> in <module>() ----> 1 @typed(A, returns=C) 2 def mismatched(a, b): 3 pass <ipython-input-4-e8ade1e4ee86> in deco(f) 7 params = inspect.getargspec(f).args 8 if not len(types) == len(params): ----> 9 raise TypeError(\"Must provide types for all parameters\") 10 annotations = {a: t for a, t in zip(params, types)} 11 annotations['return'] = returns TypeError: Must provide types for all parameters Of course, there's still the issue of things like classes that accept instances of themselves as arguments to methods. The cannonical example appears to be Nodes: class Node : def __init__ ( self , value , left = None , right = None ): self . value = value self . left = left self . right = right Since class names aren't evaluated until the entire body of the class is evaluated, it's impossible to straight up reference the class in the top level of the class, i.e.: class Node: def __init__(self, value: t.Any, left: Node, right: Node): ... This results in a NameError because of the inside out evaluation (something that has bitten me before, but was easy enough to work around in that case). I believe the current fix for this is actually inheriting from something like Generic[T], i.e.: class Node ( t . Generic [ t . T ]): def __init__ ( self , left : t . T , right : t . T ): pass Nevermind the fact that I think imposing this requirement is ridiculous not only because should types be out of the way, the implication is that I'm gaining some tangible runtime benefit by inheriting from Generic[T] — we're not, static type analysis is an \"offline\" thing. Also the problem of using my own metaclass arises. These type variables are scaffolded around using abc.ABCMeta as a base, which is fine until the fact that we can only have one metaclass in a heirarchy comes into play. Wah wah wah. I don't think that type hinting is necessarily a bad thing. However, I think as the PEP is written currently, we're sacrificing quite a bit for minimal gain.","title":"PEP 484 and Me"},{"tags":"cool stuff","loc":"http://justanr.github.io/fun-with-descriptors","text":"Once you understand their purpose and power, playing with descriptors is a lot of fun. I recently found myself needing to set an instance of a class as a class attribute on the class in question. I'm not crazy, I swear. It's for pynads' implementation of Monoids. However, doing this: class Thing ( object ): mempty = Thing () --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-1-cf5ea63fa573> in <module>() ----> 1 class Thing(object): 2 mempty = Thing() <ipython-input-1-cf5ea63fa573> in Thing() 1 class Thing(object): ----> 2 mempty = Thing() NameError: name 'Thing' is not defined Doesn't work. Wah wah wah. So my brain began turning, and I first tried this: class Thing ( object ): @classmethod @property def mempty ( cls ): return cls () Thing . mempty <bound method type.? of <class '__main__.Thing'>> At least that didn't blow up…, what if I swapped them? class Thing ( object ): @property @classmethod def mempty ( cls ): return cls () Thing . mempty <property at 0x7fd39c1601d8> Well, that didn't work either. Hm. I had hoped that composing classmethod and property together would work and that Python would magically infer what I wanted. However, Python does no such thing, unsurprisingly. So, what if I made my own classproperty descriptor? class classproperty ( object ): def __init__ ( self , method ): self . method = method def __get__ ( self , _ , cls ): return self . method ( cls ) class Thing ( object ): @classproperty def mempty ( cls ): return cls () Thing . mempty <__main__.Thing at 0x7fd39c159588> It works! Five lines of code plus another three to use it! Yeah! Except that three line use was repeated at least three times in pynads. I'm not sure about you, but I hate repeating myself. I'm gonna make a mistake somewhere (and did with List). Mainly because I was caching the instance like this: class Thing ( object ): _mempty = None @classproperty def mempty ( cls ): if cls . _mempty is None : cls . _mempty = cls () return cls . _mempty assert Thing . mempty is Thing . mempty That's great, but that's also six lines of boilerplate, which is one line longer than the implementation. ): After the benadryl wore off and I was able to think straight, what if I just moved the boiler plate into the descriptor? class Instance ( object ): def __get__ ( self , _ , cls ): return cls () class Thing ( object ): mempty = Instance () print ( repr ( Thing . mempty )) print ( Thing . mempty is Thing . mempty ) <__main__.Thing object at 0x7fd39c166e80> False Awesome. That's a much nicer use of it. The caching can also be moved into the descriptor too: class Instance ( object ): def __init__ ( self ): self . _inst = None def __get__ ( self , _ , cls ): if self . _inst is None : self . _inst = cls () return self . _inst class Thing ( object ): mempty = Instance () assert Thing . mempty is Thing . mempty And, if needed, a specific instance can be created and cached: class Instance ( object ): def __init__ ( self , * args , ** kwargs ): self . _inst = None self . args = args self . kwargs = kwargs def __get__ ( self , _ , cls ): if self . _inst is None : self . _inst = cls ( * self . args , ** self . kwargs ) return self . _inst class Thing ( object ): mempty = Instance ( hello = 'world' ) def __init__ ( self , hello ): print ( hello ) Thing . mempty world <__main__.Thing at 0x7fd39c1f5e10> And to show it's cached: Thing . mempty <__main__.Thing at 0x7fd39c1f5e10> Ta-da! Setting an instance of a class on the class as a class attribute — a sentence that is no less confusing to read now than before.","title":"Fun With Descriptors"},{"tags":"Miscellaneous","loc":"http://justanr.github.io/crazy-ideas-and-i","text":"Occasionally, absolutely crazy ideas crop up into my noggin. Recently, I've had two take up residence almost simultaneously, both related to pynads . Haskell Type Signatures Since Pynads is, nominally, a learning exercise for me to understand some concepts in functional programming — specifically in terms of Haskell — a little more deeply. I found myself wanting to using Haskell type signatures with some of my functions. The reason why is because I like Haskell's type signatures. They stay out of the way of my actual function signatures. Here's the current way Python 3 annotates functions: def my_func ( a : int , b : str = 'hello' ) -> tuple : return ( a , b ) my_func ( 1 , 'wut' ) (1, 'wut') That's so much line noise . Like what. Look at that default assignment. Like, I get why the annotations are inlined with the signature. But they're just ugly. Meanwhile, here's a similar Haskell function with the type signature: myFunc :: Int -> String -> (Int, String) myFunc a b = (a, b) That type signature is both helpful AND out of the way. However, there's one really nice thing that Python does with these annotations: my_func . __annotations__ {'a': int, 'b': str, 'return': tuple} Nice. We retain that information in a dictionary out of the way. What if we could combine these two things? from pynads.utils.decorators import annotate @annotate ( type = \"Int -> String -> (Int, String)\" ) def my_func ( a , b = 'hello' ): return ( a , b ) I like this. It stays out of the way, it uses a decorator (never enough decorators). Let's checkout the __annotations__ dict: my_func . __annotations__ {} Uh….Well, it didn't fill anything in. What did it do? Well, it attaches the signature to the docstring… print ( my_func . __doc__ ) my_func :: Int -> String -> (Int, String) That's…nice. I'm actually perfectly content with this solution currently. But wouldn't it be cool? (this is a phrase that only preceeds michief and trouble). Wouldn't it be cool if that Haskell type was somehow transformed into a Python annotations dictionary and on the other end we'd be able to inspect the annotation and get this: { 'a' : 'Int' , 'b' : 'String' , 'returns' : '(Int, String)' } {'a': 'Int', 'b': 'String', 'returns': '(Int, String)'} However, this is complicated because what if we had a higher order function? The Haskell type signature looks like this: map :: (a -> b) -> [a] -> [b] \"Take a function of type a to type b, a list of as and return a list of bs.\" Simple. How is this parsed? What if we put type class restrictions on the type: (Monad m) => m a -> (a -> b) -> m b Where m is an instance of Monad, take a m a and a function of type a to b and return a m b . What if we have multiple typeclass restrictions: (Monad m, Monoid s) => m s -> m s -> (s -> s -> m s) -> m s Maybe we're lifting mappend into a monad? Let's also pretend that this isn't a bad way of doing it as well. How do we parse this? What about \"existentially types\", aka. forall a b. (which is something I've not used, nor understand, but apparently it's useful because reasons). Of course, there are cons with this: Haskell type signatures are too complex to be matched with regular expressions. How do you write a regex for forall a b. ((a, b) -> (Char, Bool)) ? Parsers can be slow. They can be fast. But since this is a library built on understand functional things like applicatives and monads, of course I'd use an applicative/monadic combinatorial parser, which in Python will be slow. So many competing Python typing libraries. mypy seems to have gotten the BDFL nod and in fact, seems to be on track for inclusion to Python 3.5. Is it worth taking a second step and translating the parsed type signature to a type system? __annotations__ are use for things that aren't type annotations. Is this good? Is this bad? I don't know. Well, I think they should be for type signatures, but there's some cool stuff that is done with them. What about typeclass restrictions? Do we create a special entry? How do we handle collisions? Etc. Things to think about. My Broken Do Notation I implemented a rudimentary do-notation esque syntax sourced from Peter Thatcher's post on monads in Python . Here's the thing, it works…except with the List monad, because we need to repeatedly call the bound function and then flatten the list out. Except with this implementation, it hits mreturn and the whole thing blows up. Wah wah wah. But I like this style: from pynads.do import do , mreturn from pynads import List @do ( monad = List ) def chessboard ( ranks , files ): r = yield List ( * ranks ) f = yield List ( * files ) mreturn (( r , f )) #chessboard('abcdefgh', range(1,9)) Oops. But this works: def chessboard ( ranks , files ): return List ( * ranks ) >> ( lambda r : List ( * files ) >> ( lambda f : List . unit (( r , f )) )) chessboard ( 'abcdefgh' , range ( 1 , 9 )) List(('a', 1), ('a', 2), ('a', 3), ('a', 4), ('a', 5), '...54 more...', ('h', 4), ('h', 5), ('h', 6), ('h', 7), ('h', 8)) But compare to Haskell: ranks = ['a'..'h'] files = [1..8] chessboard :: [(Char, Integer)] chessboard = do r <- ranks f <- files return (r,f) It just works. Obviously, something in Pynads is implemented poorly. Wah wah wah. But what if do did something different? What if it took a look at that function and said, \"No no no, this is all wrong…let me…let me just rewrite it for you.\" And instead of actually using: def chessboard(ranks, files): r = yield List(*ranks) f = yield List(*files) mreturn((r,f)) We actually end up using: def chesboard(ranks, files): return List(*ranks) >> (lambda r: List(*files) >> (lambda f: List.unit((r,f)) )) And just rewrite the function with AST and compile it and use the new function instead. I mean if Lisp can be implemented in Python by parsing Lisp into Python AST , surely it's (relatively, being the keyword) simple to take a function with a bunch of yields and rewrite it into a function with a bunch of lambdas. Besides, wouldn't it be cool? ;) Thoughts Of course both of these are nuclear options. Well, the parser less so than rewriting AST . But, I know if I become a professional developer, at some point I'll need to have a passable understanding of parsers and ASTs (they're not even that unrelated) so instead of breaking something someone's using, why not break something I'll be the only one cursing?","title":"Crazy Ideas and I"},{"tags":"libraries","loc":"http://justanr.github.io/fizzbuzz-with-pynads","text":"from pynads import Writer from pynads.funcs import multibind from itertools import repeat from functools import partial pairs = (( 5 , 'fizz' ), ( 3 , 'buzz' )) def fizzer ( n , pairs ): fizzed = '' . join ([ buzz for fizz , buzz in pairs if not n % fizz ]) or n return Writer ( n + 1 , { n : fizzed }) res = multibind ( Writer ( 1 , {}), * repeat ( partial ( fizzer , pairs = pairs ), 15 )) print ( res ) Writer(16, {1: 1, 2: 2, 3: 'buzz', 4: 4, 5: 'fizz', 6: 'buzz', 7: 7, 8: 8, 9: 'buzz', 10: 'fizz', 11: 11, 12: 'buzz', 13: 13, 14: 14, 15: 'fizzbuzz'}) Truly an 11x Developer solution to the [ STRIKEOUT :world's] universe's premier code interview question! What is \"pynads\" All joking aside, I've been hacking together a collection of Haskell-esque tools for Python over the last few weeks. It started as a \"Well, I'll learn Haskell better this way.\" and has become…well, honestly a tool for me learning Haskell still. Check it out , I think it's nifty. A Quick Tour of Pynads pynads strives to be a pythonic form of Haskell. Which is buzzword for I did some stuff. There's: Functors Applicatives Monads Monoids Helpers All of the base classes are implemented as Abstract Base Classes which makes inheritance easy. Well, that's a lie, the root object of pynads is a concrete class that just serves as an endpoint for __new__ and __init__ . Container pynads.Container is the root object of every pynads class. It serves as a final endpoint for __new__ and __init__ as well as providing a consistent name for accessing the values held by objects in pynads . Some would say that it's a silly idea, but it works! Every class in pynads is also slotted for memory reasons since it's built around the idea of not manipulating a container but creating a new one. The only important thing to know about Container is that it defines v as a property which actually delagates to the _get_val method. Meaning that's all that needs to be overriden to get multiple values out of a container. For most subclasses of Container, the provided __init__ is fine, but it's a-ok to override it as well as the only setup that happens is setting a single attribute _v . from pynads import Container class Person ( Container ): __slots__ = ( 'name' , 'age' ) def __init__ ( self , name , age ): self . name = name self . age = age def _get_val ( self ): return { 'name' : self . name , 'age' : self . age } def __repr__ ( self ): return \"Person(name={!s}, age={!s})\" . format ( self . name , self . age ) print ( Person ( name = \"Alec\" , age = 26 ) . v ) {'age': 26, 'name': 'Alec'} Functors Functors are data types that can be mapped over with the fmap method. But calling methods isn't very Haskell like, so there's an operator that does the same thing: % . from pynads import Functor class FmapPerson ( Person , Functor ): __slots__ = () def fmap ( self , f ): return self . __class__ ( f ( self . name ), self . age ) print ( str . upper % FmapPerson ( name = \"Alec\" , age = 26 )) Person(name=ALEC, age=26) Of course, you could just use FmapPerson.fmap but that's not very Haskellic. pynads also exports a funcs name space that contains functional shortcuts to some of these (though f % functor doesn't get much shorter). In this case, there's also: from pynads import funcs print ( funcs . fmap ( str . upper , FmapPerson ( name = \"Alec\" , age = 26 ))) Person(name=ALEC, age=26) Every class in the pynads.concrete namespace is a functor except for Mempty (we'll get there!). Applicatives Applicative Functors are functors that hold functions and can then be applied to other Applicatives/Functors. Every class (except for Mempty ) in pynads.concrete is also an Applicative! from pynads import Just print ( Just ( lambda x : x + 2 ) * Just ( 2 )) Just 4 Yeah, like Haskell there's an infix operator. Except of <*> I just dropped the angle brackets because Python doesn't let you define custom operators (easily). It also combines nicely with % because they have the same precedence level! print (( lambda x : lambda y : x + y ) % Just ( 4 ) * Just ( 6 )) Just 10 BOOM ! Mind blown! Whaaaaaat. Applicative uses the abstract method apply to determine how * operates. Just inherit from Applicative, define fmap and apply and boom, you're on your way. Well, you also need the unit method — which is a class method for all pynad types, but that's not a requirement — which knows how to put a value in a minimal context. But wait, what if you have a curried function and you stuffed it into a Just and now you don't want to write out just_f * just_v1 * just_v2 ... . Sure, you could think \"Well, what if I used reduce(operator.mul, *justs) \" But I thought of that already. add_three_together = lambda x : lambda y : lambda z : x + y + z print ( funcs . multiapply ( Just ( add_three_together ), * [ Just ( x ) for x in range ( 1 , 4 )])) Just 6 If you're mind isn't blown yet, it's because I haven't revealed… MOOOOOOOOONAAAAAAAADS !!!!! Monads get a bad rap because they get all sorts of overblown explainations. You want to know a what a monad is? It's another way to compute things. It's a glorified container with a special method. You have a value in a monad, a function that takes a regular value and returns a monad and you bind them together. That's it. Literally all there is to it. from pynads import Nothing inc_if_odd_else_nothing = lambda x : Just ( x + 1 ) if not x & 1 else Nothing print ( Just ( 2 ) >> inc_if_odd_else_nothing ) Just 3 The Maybe monad (which consists of the Just and Nothing data types) is basically a glorified if expression. That's it! The bind operation will detect if you have a failure in your computation and short circuit it. It's essentially an abstraction over this: def safe_func ( x ): if x is None : return None else : return x + 1 print ( safe_func ( 1 ), safe_func ( None )) 2 None Notice how that didn't cause a nasty AttributeError, because None doesn't have attributes? That's all Maybe lets you do (this behavior is mimicked in its fmap and apply: fmap(f, Nothing) and apply(ap_f, Nothing) both return you a Nothing). Nothing is extraspecialsauce because it's a singleton. It's basically a monadic None. Actually, it is a monadic None because it represents…well, Nothing. If you've got more binds than editor columns, then there's something for you as well! print ( multibind ( Just ( 1 ), * repeat ( lambda x : Just ( x + 1 ), 5 ))) Just 6 Before you start thinking, \"Well, monads are just glorified if expressions\" becuase that's missing the point, Monads are the abstraction abstraction. They represent a way to compute something by abstracting away how it's computated. There's the Writer monad above which is a value in a monadic context but it also keeps some extra side-output as well. Instead of us trying to track this extra information ourselves, Writer says, \"Hey, I'll handle it for you!\" It just wants a function that accepts a value and returns a Writer. But here's the really cool thing, I didn't have to use a dictionary. It could have a list, or a string or an integer, or a custom class! I hear, \"But how!\" Through the power of… Monoids! So monoids are pretty cool. They're a set of something, a \"zero\" and a binary operator that's transative. Lots of things form monoids. Numbers are monoids! There's two ways to make numbers monoids: with 0 and + , with 1 and * . However, pynads is lazy and only defines the first…sorry, hard choices were made. Wait, \"a zero value\" but 1 != 0 . That's missing the point, a zero value is a value that doesn't change the input when combined with the binary operator. x * 1 == x . But Python's \"primitive\" types all form monads! list is a monoid. That's the zero value right there and it's binary operator would be list.extend if it was actually a binop. dict is a monoid with {} and dict.update bool is a monoid with False and | \\ or set and frozenset are also monoids with their empty instances and | str is a monoid with '' and + (boooo using + to combine strings! but whatever) Tuple, Complex and Float are also monoids in exactly the ways you expect. There's a catch though: When combining tuples, you actually get a list back. I'll probably rectify this at a later point, but it's just something to live with right now. pynads also defines [ STRIKEOUT :two] three monoids of its own: List (the monadic form of tuple ), Map (the applicative form of dict ) and Mempty (which we're getting to!). Making your own monoid is easy, and you've probably done it before (just not in this fashion). Just inherit from pynads.Monoid create a mempty attribute (it won't let you without it through __new__ hackery) and the mappend method for combining two instances of your monoid. Let's assume we want a real tuple Monoid. We'd do it like this: from pynads import Monoid # also inherits from Container # so we get all the Container goodness for free class MTuple ( Monoid ): mempty = () def __init__ ( self , * vs ): super ( MTuple , self ) . __init__ ( vs ) def mappend ( self , other ): # type checking optional if not isinstance ( other , MTuple ): raise TypeError ( \"Can only mappend MTuple with MTuple\" ) return MTuple ( * ( self . v + other . v )) def __repr__ ( self ): return \"MTuple{!r}\" . format ( self . v ) print ( MTuple ( 4 , 5 ) + MTuple ( 6 , 7 )) MTuple(4, 5, 6, 7) pynads.Monoid overrides + to be a shortcut to mappend . That's all well and good, but why other than have a unified way of combining values?! Because we get a way to reduce a list of monoids into a single value for free! print ( MTuple . mconcat ( MTuple ( 1 ), MTuple ( 2 ), MTuple ( 3 ))) MTuple(1, 2, 3) Monoid.mconcat actually delegates to the mappend method and essentially looks like reduce(cls.mappend, monoids) . That's it. That's all there is. But you can define your own mconcat to get performace bonuses if you need to. from itertools import chain class CMTuple ( MTuple ): def __iter__ ( self ): return iter ( self . v ) @classmethod def mconcat ( cls , * MTuples ): return CMTuple ( * chain . from_iterable ( MTuples )) print ( CMTuple . mconcat ( CMTuple ( 1 ), CMTuple ( 2 ), CMTuple ( 3 ))) MTuple(1, 2, 3) pynads.List and pynads.Map take advantage of this to create only one intermediate object rather than a bunch. pynads will also let you treat the builtin types as monoids as well through pynads.funcs.monoid namespace which has four functions we're interested in: mempty , mappend , mconcat and is_monoid . mempty returns the \"zero\" value for a type, mappend knows how to combine types, mconcat knows how to combine an iter of types into a single one and is_monoid knows if something is monoidal or not (generally, it doesn't declare decimal.Decimal to be a monoid but this is because I didn't want to add a special case — special cases beget special cases). This is done through introspection of types and abstract base classes (to make the type introspection [ STRIKEOUT :more acceptable] less painful). from pynads.funcs import monoid print ( monoid . mempty ( list ())) print ( monoid . mappend ({ 'a' : 10 }, { 'b' : 7 })) print ( monoid . mconcat ( \"hello\" , \" \" , \"world\" )) print ( monoid . is_monoid ( set ())) [] {'a': 10, 'b': 7} hello world True The monoid namespace is just a nice interface to the nasty plumbing that lives in pynads.utils.monoidal . It's pretty gross and actually probably pretty fragile, but it works! IT WORKS !!! Mempty So here's the thing that lets Writer do its little trick by accept any monoid as a potential log. I can't know ahead of time what you're going to use to keep track of stuff with Writer — I've drank plenty of spice water, but I've yet to develop prescient abilities. And rather than making a bunch of subclasses specialized to handle a dictionary and a list and a string and a blah blah blah and forcing you to make your own for WriterLogWithMyFirstMonoid I decided to create a mempty monoid — Mempty . It's not an original idea. Really, it's kind of a dumb object. It's a singleton, so that's a strike against it (two singletons in my library, my god!). It doesn't actually do anything. It just sits around, taking up space until a real monoid comes along and scares it off. It's mempty value is actually itself! It's mappend just returns whatever its mappended with. And it's mconcat filters out any Mempty values before trying to mconcat the remaining values (you get a Mempty if mconcat an iter of Mempties). There's even an __iter__ method that yields from an empty tuple! What's going on! In Haskell, mempty can be used a place holder and Haskell knows to do the right thing already. However, we have to teach Python how to use a placeholder and silently step it out of the way when a real value comes along. I suspect that this is similar, maybe, to how Haskell handles it, but I've not dug at all. from pynads import Mempty print ( monoid . mempty ( Mempty )) print ( Mempty + 4 ) print ( monoid . mconcat ( Mempty , { 4 }, { 5 }, Mempty )) Mempty 4 {4, 5} So Writer's little trick of allowing any Monoid be the log is really just, \"I have this dumb thing that pretends to be a Monoid as a default log.\" Helpers I won't explore this section indepth because pynads.funcs has a whole host of helpers, with more being added as needed. There's helpers for making constants, composing functions, an identity function (as in lambda x: x not id(x) ), turning values into Monads. There's also pynads.utils if you need to plumb some of the internal workings like compatibility (did I mention all the tests pass 3.4 and 2.7?!) or how monoids are determined on things that aren't explicitly monoids. That's it! Well, not really. There's about five other monads implemented that I didn't cover (like Reader, which was a pain but also a lot of fun to implement). There's more coming as well. Not just monads, but other things. A proof of concept for do notation via abusing coroutines exists in the repo (lifted from a blog called Valued Lessons which served as partial inspiration for this project). And maybe Monad Transformers if I work the gumption up for it (probably yes). And who knows what else.","title":"Fizzbuzz With Pynads!"},{"tags":"tips and tricks","loc":"http://justanr.github.io/musings-on-problem-solving","text":"I just started reading a book on algorithms and data structures in Python. I've also commited myself to reading every chapter, taking notes and doing every exercise at the end of each chapter (okay, Chapter 1 is all \"This is Python, this is the syntax, here's how you loop…\" so I skimmed it) except for the ones I can't reasonably fit in a single IPython Notebook cell (I'm not trying shove a calculator into a notebook) or are just plain boring (\"Print a statement 100 times!\" — seriously, this is an exercise). Most of the problems are okay, made me think some. Some are ones I've done before. Others made me think about how to decompose the problem into smaller, more managable bits. And I'd like to share my process on one in particular. Our Task To paraphrase the exercise: We want to accept three integer inputs and determine if either a = f(b,c) or f(a,b) = c , where f is some mathematical function (maybe addition, modulo but log and pow are candidates as well). Right now, if you feel this is a complex challenge, I want you attempt to code it and come back and see how your code and my code differs. If you get stuck, don't feel bad and come along for the ride. The Steps The first thing I like to try to do is break a problem down into steps. If a step is ambigous, I try to decompose the step into smaller bits. Eventually, I have kinda procedural pseudo-code. Collect Operations Collect Integers Map inputs to operations Check if at least one operation returned true Put the whole thing together. To me, those are pretty clear cut, atomic steps. Sure, they're composed of smaller steps internally, but they represent a single unit of work each function of the program needs to do. Step 5 is usually implied, but I find it's nice to list anyways. Step 1: Collect Operations We'll just make a list of all the operations we want to check against. Since Python has first class functions (meaning functions can be treated like data), this is really easy. We'll also make a simple root function for calculating things like cubic roots as well. from operator import add , sub , mul , mod , truediv from math import log # roots are just fractional exponentials when you get down to it # ...or maybe exponentials are fractional roots... def root ( base , power = 2 ): return base ** ( 1 / power ) operations = [ add , sub , mul , mod , truediv , log , root , pow ] Super simple. Now, let's move on. Step 3: Map Inputs To Operations \"Alec,\" you say, \"You skipped a step!\" And I did. Because I/O is nasty and I'd like to do it last. For now, let's assume the integers we collected are 1, 2 and 3. integers = [ 1 , 2 , 3 ] For this step we need to make our operation to a potential output. Basically: f(a, b) == c . Again, this is easy. Even if we decide to check if a == f(b,c) in the same step. Like I said before, Python's functions can be treated just like data and this includes passing them as arguments to other functions. def either_side_true ( op , a , b , c ): return a == op ( b , c ) or op ( a , b ) == c Step 4: Determine if any check returned True There's many ways we could do this. But the key here is the word any — which happens to be a Python built-in function. What any does is accept an iterable of values and checks if at least one of them is True. There's also its sibling all which checks that none of the values are False. What's really nice about any and all is they allow collapsing a bunch of and / or groupings in an if statement into a single expression. Say you wanted to check if any variable in a given group was equal to something. You might do (ignoring that you'd actually have a list and do if 1 in list_of_vars: )… a = 2 b = 1 c = 5 if a == 1 or b == 1 or c == 1 : print ( \"We've got a winner!\" ) We've got a winner! …but with any we can simply do this… if any ( v == 1 for v in [ a , b , c ]): print ( \"I said We've got a winner!\" ) I said We've got a winner! Of course, the true power of any and all comes when you combine them with generators, which will allow you to lazily compute your check. These aren't a catch all, sometimes you want to store the output of a check or you already have a list built, in which case x in my_list is superior, but these should be tools you consider from time to time. Now, we want to check if any of our operations returns True if a == f(c,b) or c == f(a,b) . if any ( either_side_true ( op , * integers ) for op in operations ): print ( \"Heck yeah at least one did.\" ) Heck yeah at least one did. Alternatively, we could store that as a separate function if we needed to fiddle with it more, but for this case, I feel that's overkill. Step 2: Collect Integers Okay, now we'll worry about actually grabbing some input from the user. We need to collect three integers, probably using STDIN . But they could come from a file, a database or anywhere. But let's focus on just grabbing them from STDIN . def collect_ints_from_stdin ( n = 3 ): ints = [] while len ( ints ) < n : try : v = int ( input ( \"Please type an integer: \" )) except ValueError : continue else : ints . append ( v ) return ints Normally, I'd wince at a while loop used to build a fixed length list, but we have to trouble with users doing something like trying to enter the number a (well…you could use hex…but who does that?). Since we're just passing on the error, our list of integers could end up short! You might also notice that I've used try/else . I am of the opinion that a try block should be isolated to just the code that could raise an exception. except handles cleaning up the exception. And then there's else which allows us to run code on the condition we didn't encounter a problem. Mentally, if you replace except ValueError with if catch(ValueError) where catch is some magic framehack or something, it begins to make sense. I'll also note, for completion's sake that there's also an optional finally clause which lets us run code regardless of an exception occuring. But the full try/except/else/finally concept is for another time. Step 5: Put it altogther Finally, we'll take these individual components and piece them together into a cohesive answer. def answer ( operations , num_ints = 3 , collector = collect_ints_from_stdin ): ints = collector ( num_ints ) if any ( either_side_true ( op , * ints ) for op in operations ): print ( \"Heck yeah at least one did!\" ) else : print ( \"D: Nope!\" ) If you add if __name__ == \"__main__\": answer(operations) and count spacing and imports, the whole thing's less than forty lines! The Take Away Break a problem down into steps as small as possible Small, functional pieces are more manageable Breaking down a problem is something that seems so obvious but it's a skill that needs to be practiced and honed just like any other. After all, programming isn't so much about code as solving problems. And part of solving problems is finding the smaller problems hidden inside them. What looked liked a complicated task on the surface, ended up being just five simple tasks on the inside. That's not to say that every problem will immediately break down to simple tasks. Sometimes you'll end up with a list of steps that's nested five levels deep! But if you tackle each step one at a time, sprinkle in some Python idioms and a touch of abstraction, your application will come together. The other, writing smaller more functional code pieces allows your code to be modular. What if tomorrow, we want to grab the integers from a database? Well, since collect_ints_from_stdin is separated from the main function, we can easily. Just build a callable that queries the database for integers and feeds them into the operations. Or if we need to change from integers to strings, we just swap out the collector and the operations and move on with our day — or better, pass a callback to coerce the type for us! But consider if we had just written a thirty line function that did all this? Ugh, it'd be a nightmare to untangle the pieces. Honestly, we'd probably have two functions that did the same thing, but just grabbed data from other locations. And then two more functions because maybe we need to work on strings. Quickly, this spirals out of control as the number of functions grows at (Data Types) * (Sources). We have four sources and four data types? That's 16 functions! I don't want to write the same function 16 times and I doubt you do either. Instead we can write four functions to grab the data, pass our coercion callback and maintain the lists of operations. Going Further There are problems. What happens when we input 0 and our checks make it to the division, modulo, log or root operations? We get a nasty exception. :( But handling it is easy (hint: ZeroDivisionError and ValueError would needed to be caught). Also, we should be able to tell the user which operations returned true. Of course, I'm not going to play my full hand. I'll leave these as an exercise to you to figure out.","title":"Musings on Problem Solving"},{"tags":"tutorials","loc":"http://justanr.github.io/understanding-foobar","text":"One of my favorite shows is \"How It's Made\" — my enjoyment mostly stems from learning how stuff is made, but the narrator's cheeky puns and jokes certainly add to it. But something I enjoy more than knowing how stuff is put together, is knowing how things work. I don't know what it is, but I have this childlike fascination with opening things up and learning how it fits together, what each part. That was one of my favorite things about my brief stint (a whooping six months!) in the automative service industry: understanding, a little better, how cars work. It certainly opened my eyes to all the work that goes into even simple automotive repairs. Sadly, I no longer work on or with cars, I do still fiddle some with mine though, and if anyone has a good link to how a transmission — manual or automatic — actually works, I'd be thrilled! But this has left me with a hole in my life. One I've recently begun to fill with how Python operates under the hood — so to speak. While my skills with C — which basically amount to printf and for loops — leave me woefully unprepared to examine much of the source, I can examine the surface parts. To use a car analogy, if reading the C source for Python is repairing a damaged block or transmission, examining how Python works is more similar to replacing motor mounts and broken belts (something I'm regretfully too familiar with on my CRV ). Whereas reading someone else's Python is like doing your own fluid changes. Flawed analogies aside, I'd like to more fully examine how Python objects work and what it really means to call foo.bar() . As a forewarning, this knowledge is great for understanding what's happening, but it's not crucial knowledge to working with classes and objects in the regular sense. All the things I will discuss here deal with how Python 3 handles them. Python 2 is slightly different. Building a Class To talk about Python's data model and how it relates to classes and objects, we should first write a class. It so basic as to wonder why we're doing it. The point is, rather than examine some fictional class or object, why not have one of our own to open up and poke at? class Baz : def __init__ ( self , thing ): self . thing = thing def bar ( self ): print ( self . thing ) That's an extremely basic object. The initalizer takes a single argument a method that prints it out. Of course, we need to instantiate it for us to get use out of it. foo = Baz ( 1 ) Already, there's some mechanisms at work for us. I don't want to get too deep into class creation, but the short take away is the implicit __new__ classes inherit from object handle object creation and __init__ simply sets the init ial state of the object for us.. Delving into __new__ hooks into dealing with metaclasses, which is a topic for another time. What I want to focus on today is what happens when we call foo.bar() Classes and Objects You'll often hear that objects and classes in Python are simply nothing more than a pile of dictionaries with dotted access. This obtuse phrasing confused me for a long time and it wasn't until I began asking, \"How the heck does self actually get passed?\" that I began to understand. Asking this began me down a rabbit hole that lead me to descriptors and __getattribute__ and what they do. The Dict All classes in Python have an underlying __dict__ and nearly every instance does as well. The first step to foo.bar() is understanding that methods live at the class level. print ( 'bar' in Baz . __dict__ ) print ( 'bar' in foo . __dict__ ) True False Methods are entries in the class's underlying __dict__ but not in the instance's. Because of this, most Python objects can remain relatively small, they simply store their state rather than all of their available methods as well. What does this method look like in the dictionary? from inspect import isfunction , ismethod print ( isfunction ( Baz . __dict__ [ 'bar' ])) print ( ismethod ( Baz . __dict__ [ 'bar' ])) print ( Baz . __dict__ [ 'bar' ]) True False <function Baz.bar at 0x7f1d05a87ea0> We can see that in the class's dictionary, methods are stored as functions and not as methods. It's reasonable to infer that methods are actually functions that operate on class instances. From here, we can imagine that behind the scenes Baz . __dict__ [ 'bar' ]( foo ) 1 Attribute Access The next piece of the puzzle is how Python handles attribute access. If you're not familiar with how Python attribute look up happens, in short, it looks like this: Call __getattribute__ Is the attribute in the object __dict__ ? No? Is the attribute in the class's __dict__ ? No? Is the attribute in any of the parent classes' __dict__ ? No? Call __getattr__ if present. Else, raise an AttributeError Python starts at the bottom, calling __getattribute__ . This what actually allows the dotted access. You can think of the . in foo.bar to be implicit call to this method. This method translates dictionary look up to dotted access and invokes the rest of the chain. Since we already know that methods live in the class's __dict__ and methods are functions that act on the instance, we'll fast forward to there and extrapolate. Since methods are functions that live in the class's dictionary and act on instances and __getattribute__ is an implicit transformation from attribute to dictionary look up, we can infer that method calls look like this behind the scenes: Baz . bar ( foo ) 1 Methods vs Functions So far so good. All this is pretty easy to grasp. But there's still burning question of how the heck is self (or rather foo ) being passed to our methods. If we examine Baz.bar and foo.bar both, we can see there's a transformation going on somewhere. print ( Baz . bar ) print ( foo . bar ) <function Baz.bar at 0x7f1d05a87ea0> <bound method Baz.bar of <__main__.Baz object at 0x7f1d05a88208>> Python is some how transforming our function that lives in Baz ‘s dictionary into a method tied to our instance foo . The answer lies in the descriptor protocol. I've written about it else where, and it's probably time to revise it again with my recent understanding. But essentially, descriptors add another rule to our attribute look up. Just before the __getattr__ call: If we recieved a descriptor, call the __get__ method on the descriptor. This is our missing link. When a function is declared in the class, not only is it placed in the class's dictionary it's also wrapped by a descriptor. Or more accurately, a non-data descriptor because it only defines the special __get__ method. The way descriptors work is by intercepting lookup of specific attributes. The Descriptor likely has a passing resemblance to this (of course, implemented in C): from types import MethodType class MethodDescriptor : def __init__ ( self , method ): self . method = method def __get__ ( self , instance , cls ): if instance is None : return self . method return MethodType ( self . method , instance ) So, our initial thought of what foo.bar() looks like under the covers was wrong. It more accurately resembles: Baz . __dict__ [ 'bar' ] . __get__ ( foo , Baz )() # if we inspect it we see the truth print ( Baz . __dict__ [ 'bar' ] . __get__ ( foo , Baz )) 1 <bound method Baz.bar of <__main__.Baz object at 0x7f1d05a88208>> And in fact, if we put our imitation method descriptor into action, it works similarly to how object methods do. def monty ( self , x ): print ( x ) class Spam : eggs = MethodDescriptor ( monty ) ##of course, it's also useable as a decorator @MethodDescriptor def bar ( self ): return 4 ham = Spam () # a lie if I ever saw one print ( Spam . eggs ) print ( ham . eggs ) ham . eggs ( 1 ) print ( ham . bar ()) <function monty at 0x7f1d045cef28> <bound method Spam.monty of <__main__.Spam object at 0x7f1d05a780b8>> 1 4 The reason we see a function when we access the bar method when we access it through the class is because the descriptor has already run and decided that it should simply return the function itself.","title":"Understanding foo.bar()"},{"tags":"tutorials","loc":"http://justanr.github.io/observer-pattern-with-descriptors","text":"Recap In the last post about descriptors I introduced the concept of building an observer pattern with descriptors, something Chris Beaumont almost teases with in his Python Descriptors Demystified . But, I feel he left a lot on the table with that concept. Before delving deep into the code (and this post is going to be very code heavy), let's recap what we learned last time: Learned about how Python handles attribute access on objects. What the descriptor protocol is and how to briefly implement it Stored the data on the object's __dict__ Used a metaclass to handle registering the descriptors for us. And now for a little bit of code dump to get it active in this notebook as well as reminding us what it looks like: class Descriptor : def __init__ ( self , name = None , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . name = name class checkedmeta ( type ): def __new__ ( cls , clsname , bases , methods ): # Attach attribute names to the descriptors for key , value in methods . items (): if isinstance ( value , Descriptor ): value . name = key # really we should use super rather than type here return super () . __new__ ( cls , clsname , bases , methods ) Callbacks Callbacks are simply actions that run in response to something. They allow external code to react and hook into your code. This style of programming is very common, for example, in Node.js. These can be utilized in Python as well. For now, I'm going to stick with my business crucial to_lower as our callback to give an example before moving on to actually working with the observer pattern. # pretend this lives at in a package called critical # and actually does something really useful def to_lower ( value ): return value . lower () def print_lower ( value ): print ( to_lower ( value )) #from critical import print_lower def my_business_logic ( value , callback ): remove = 'aeiou' value = '' . join ([ c for c in value if not c . lower () in remove ]) callback ( value ) return value my_business_logic ( 'Alec Reiter' , callback = print_lower ) lc rtr 'lc Rtr' Now, the callback could have done anything like updating a database, sending a tweet or simply plug it into a grander processing framework. Node.js uses callbacks for things like error handling on view functions. This is just to give an idea of what's happening in a basic sense. Your code runs and then sends a request to the callback for more action. Implementing call back descriptors is pretty easy. class CallbackAttribute ( Descriptor ): def __init__ ( self , callback = None , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . callback = callback def __set__ ( self , instance , value ): instance . __dict__ [ self . name ] = value if self . callback : self . callback ( instance , self . name , value ) def frobed_callback ( instance , name , value ): print ( \"Set {} on {!s} to {}\" . format ( name , instance , value )) class Thing ( metaclass = checkedmeta ): frob = CallbackAttribute ( callback = frobed_callback ) def __init__ ( self , frob ): self . frob = frob foo = Thing ( frob = 4 ) Set frob on <__main__.Thing object at 0x7f0d2c2c6358> to 4 Of course this is an incredibly limited callback descriptor, we're limited to only one callback that's set at class definition time. But it's merely to serve as an example of what's to come. Observers According to wikipedia, The Observer Pattern is a software design pattern in which an object, called the subject, maintains a list of its dependents, called observers, and notifies them automatically of any state changes, usually by calling one of their methods. It is mainly used to implement distributed event handling systems. And according to the Gang of Four: Define a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automatically. The Gang of Four moves on to state that observers and subjects shouldn't be tightly coupled because it reduces the ability to reuse them else where. Put plainly, your subject shouldn't have hard coded logic to call to specific observers. Rather, you should be able to register instances of observers onto an object (or class) and have it call out to them programmatically. You might run into other names such as Event Handler, PubSub/Publisher-Subscriber, or Signals. These are all variations (to my best understanding) on the pattern with minute but important differences. I won't delve into them, but the take away is that all four of these follow the same basic pattern: An object hooks callbacks which run when they're notified of something. An easy implementation of this would look like this: from abc import ABCMeta , abstractmethod class SubjectMixin : \"\"\"Mixin that will allow an object to notify observers about changes to itself.\"\"\" def __init__ ( self , observers = None , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . _observers = [] if observers : self . _observers . extend ( observers ) def notify ( self ): for observer in self . _observers : observer . update ( self ) def add_observer ( self , observer ): if observer not in self . _observers : self . _observers . append ( observer ) def remove_observer ( self , observer ): if observer in self . _observers : self . _observers . remove ( observer ) class ObserverMixin ( metaclass = ABCMeta ): \"\"\"Mixin that will allow an object to observe and report on other objects.\"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) @abstractmethod def update ( self , instance ): return NotImplemented An initial attempt at this pattern will use inheritance (or interfaces if you're using something like PHP or Java where single inheritance is the only option). The pattern is simple: We store observers in a private (or at least as private as Python allows) list When we need to notify the observers, we do so explicitly by hitting all of them and their update method Observers are free to implement update in whatever way, but they must implement it. A simple implementation might look like this. class Person ( SubjectMixin ): def __init__ ( self , name , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . name = name class PrintLowerNoVowels ( ObserverMixin ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) def update ( self , instance ): remove = 'aeiou' value = instance . name . lower () value = '' . join ([ c for c in value if not c in remove ]) print ( value ) plnv = PrintLowerNoVowels () me = Person ( name = \"Alec Reiter\" , observers = [ plnv ]) me . notify () lc rtr This is generally how it's implemented — at least in most of the articles I read. It's also possible to automate the notification via property . Say, we wanted to notify the observers every time we change the name attribute on a Person instance. We could write that logic every where. Maybe apply it with a context manager or decorator. But, tying it to the object makes the most amount of sense. class Person ( SubjectMixin ): def __init__ ( self , name , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . __name = None self . name = name @property def name ( self ): return self . __name @name.setter def name ( self , value ): if value != self . __name : self . __name = value self . notify () me = Person ( name = \"Alec Reiter\" , observers = [ plnv ]) lc rtr If we're concerned about automatically notifying the observers any time an attrbute is changed, we could just override __setattr__ to handle this for us. Which circumvents the needs to write properties for every attribute if this is the only action we're concerned with. It's super easy to implement as well. class Person ( SubjectMixin ): def __init__ ( self , name , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . name = name def __setattr__ ( self , name , value ): super () . __setattr__ ( name , value ) self . notify () me = Person ( name = \"Alec Reiter\" , observers = [ plnv ]) lc rtr And that's all well and good. Not to mention a good deal less complicated that what I'm about to delve into. But it's also less fun for me. I'm not going to advocate for one of these implementations over the other except to say the one I'm going to focus on will offer a much finer grain of control. Watching specific attributes However, if we're concerned with monitoring specific attributes for changes, descriptors are the correct way to handle this. Why bother emitting an event every time age is changed if we only care about name or email ? The first step is to identify the logic we'd end up repeating in each property and moving that into a seperate object. We'll call this new class WatchedAttribute . class WatchedAttribute ( Descriptor ): def __init__ ( self , name = None , * args , ** kwargs ): super () . __init__ ( name , * args , ** kwargs ) def __set__ ( self , instance , value ): if self . name not in instance . __dict__ or value != instance . __dict__ [ self . name ]: instance . __dict__ [ self . name ] = value instance . notify () class Person ( SubjectMixin , metaclass = checkedmeta ): name = WatchedAttribute () def __init__ ( self , name , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . name = name me = Person ( name = \"Alec Reiter\" , observers = [ plnv ]) me . name = \"Alec Reiter\" lc rtr Now, we can add multiple attributes that are watched without rewriting the property each time to change the variable name. If we split the name attribute into first and last names, if we add an email attribute it's easy. Just add another WatchedAttribute entry on the class level and set it in __init__ . But I feel we can improve on this pattern as well. There's two big things I'm not a fan of with this implementation: We manipulate the underlying dictionary to store the values. The Subject is responsible for notifying the Observers. We can fix both of these things, but the first will take us down a side road. Alternative Data Store The first issue is trickier. We need to relate instances to values without creating a mess we'll have to clean up later, or creating a memory leak that will absolutely murder a long running process. The most effective way of handling both of these is using weak references. References CPython (the implementation I'm using) utilizes reference counting to determine if an object should be garbage collected. When an object's reference count drops to 0, it's space in memory can be reclaimed by Python for use else where. Sometimes we only want to hold a reference to an object but not so tightly it won't be garbage collected if we forget about it. Consider this: print ( me ) registry = { \"me\" : me } <__main__.Person object at 0x7f0d2c2c64e0> Storing instances in a dictionary as keys or values (or in a list or set) as a form of caching is extremely common. But if we remove all the other instances of the object laying around… del me …that reference is left hanging around: me = registry [ 'me' ] print ( me ) <__main__.Person object at 0x7f0d2c2c64e0> Before this gets too side tracked into weak references, I want to note that they're not a silver bullet and require a little more knowledge about Python to use efficienctly. You can still shoot your foot off with them. In this case, we're not using them prevent cycles but to instead maintain a cache. Peter Parente wrote about weak references on his blog and while some of the information is out dated (the new module was deprecated in 2.6 and replaced with types ), it's still relevant to understanding what weak references are. And Doug Hellman explored the weakref module in his Python Module of the Week series. But the short of it is that an instance of WeakKeyDictionary , WeakValueDictionary or WeakSet will prevent this. Most things can be weak referenced — the documentation goes into detail about what can be: \"class instances, functions written in Python (but not in C), instance methods, sets, frozensets, some file objects, generators, type objects, sockets, arrays, deques, regular expression pattern objects, and code objects.\" When you're attempting to use WeakKeyDictionary or WeakSet , the object must meet one more requirement: hashable. So objects like list or dict , even if they were implemented in Python, can't take advantage of these structures. However, outside of a few corner cases, this restraint won't affect us here. Implementing it is incredibly easy. from weakref import WeakKeyDictionary class WatchedAttribute ( Descriptor ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . values = WeakKeyDictionary () def __get__ ( self , instance , cls ): return self . values [ instance ] def __set__ ( self , instance , value ): if instance not in self . values or value != self . values [ instance ]: self . values [ instance ] = value instance . notify () class Person ( SubjectMixin ): name = WatchedAttribute () def __init__ ( self , name , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . name = name me = Person ( name = \"Alec Reiter\" , observers = [ plnv ]) lc rtr You'll notice the metaclass we were using before is also gone now. That's because since we're storing the information on a cache inside the descriptor, it no longer needs to worry about what name it's being held under. Descriptors as Subjects The next issue was moving the publishing of events out of the main object. The main reason for this would be to notify only certain subscribers when an attribute changes but not all of them. This explores what happens when access a descriptor through the class and not an instance. Meaning answering, \"What does cls (or type) do on __get__ ? Accessing the Descriptor Since descriptors are objects that just happen to follow a certain protocol that doesn't mean they can't have other methods on it. Or even follow multiple protocols. An object could be both a descriptor and an iterator for example. However, getting to these other methods can be tricky. We obviously can't do it through an instance, Python resolves that access to the __get__ method and returns a value. This means we have to go through the class. But the way our descriptor is set up, it'll blow up when an instance isn't passed to it. We could simply return the instance of the descriptor when an instance isn't passed…would it work? Spoilers: It does. So we can fully move the registration of observers and notification out into the descriptors and our SubjectMixin can be redefined to work with our descriptor. Actually, we end up redefining the Descriptor and WatchedAttribute classes as well. Forewarning, this is a bit of a code dump. from weakref import WeakSet class SubjectMixin : def __init__ ( self , observers = None , * args , ** kwargs ): self . _observers = WeakSet () super () . __init__ ( * args , ** kwargs ) if observers : self . _observers . update ( observers ) def notify ( self , instance ): for observer in self . _observers : observer . update ( instance ) def add_observer ( self , observer ): self . _observers . add ( observer ) def remove_observer ( self , observer ): if observer in self . _observers : self . _observers . remove ( observer ) class CachingDescriptor ( Descriptor ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . _instances = WeakKeyDictionary () def __get__ ( self , instance , cls ): if instance is None : return self return self . _instances [ instance ] def __set__ ( self , instance , value ): self . _instances [ instance ] = value class WatchedAttribute ( CachingDescriptor , SubjectMixin ): def __init__ ( self , observers = None , * args , ** kwargs ): super () . __init__ ( observers = observers , * args , ** kwargs ) def __set__ ( self , instance , value ): super () . __set__ ( instance , value ) self . notify ( instance ) class Person : name = WatchedAttribute () def __init__ ( self , name , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . name = name Person . name . add_observer ( plnv ) me = Person ( name = \"Alec Reiter\" ) lc rtr There's some subtle changes going on here that you might miss unless you explicitly diff the preceeding implementation of SubjectMixin with this one. The observer container is changed from a list to a WeakSet . Both are iterable, which means notify doesn't change (at least due to this). WeakSet plays off both the strengths of sets (which only contain unique items) and weak references. The only thing that WeakSet won't handle is keeping the observers in any particular sort of order and dealing with unhashable types — neither of which affect us. You'll notice that adding elements to a set is slightly different from a list, so it's not a complete drop in replacement. I will note I went back and forth between using WeakSet and a regular set . Mostly because if we remove all other references from an observer, do we intend to still have the observer still process requests? My thoughts on the matter is no, the observer should be considered dead. In other cases, the goal could be to have \"anonymous\" observers — objects that are created and immediately injected into the framework rather than assigned to a name and passed in. If this is the desire, than WeakSet wouldn't keep the object from being immediately garbage collected. I'll leave the pros and cons of both approaches as an exercise to the reader. ;) The next subtle difference is that SubjectMixin.notify now accepts an instance explicitly. Since we've displaced this logic to the descriptor, passing self to it ends up passing the instance of the descriptor rather than an instance of the class it's attached to. Other than that, it's just a matter of knowing how multiple inheritance works. Which is a completely separate matter best left to another time. It involves liberal use of super to say the least. The short of it is that WatchedAttribute combines the methods and data from both Descriptor and SubjectMixin together. Meaning Descriptor can worry about being a descriptor that stores information in a weak ref dictionary. And SubjectMixin can worry about being the basis for observed subjects — it's applicable for both descriptors and other objects. WatchedAttribute just overrides how Descriptor.__set__ operates (or rather extends it if you want to split hairs) to combine the two fully. Going Further We could, of course, go further to registering observers for every instance of an object with the WatchedAttribute and specific instances as well. Implementing this is a just a mite trickier, but not terribly. The first step is to imitate the behavior of collection.defaultdict in WeakKeyDictionary . Emulating defaultdict is pretty straight foward and just depends on defining __missing__ , setting a hook for it in __getitem__ and providing a constructor. The reason for building this is to utilize WeakSet as a way to hold onto observers for us that are local to a particular instance. class WeakKeyDefaultDict ( WeakKeyDictionary ): def __init__ ( self , default_factory = None , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . default_factory = default_factory def __getitem__ ( self , key ): try : return super () . __getitem__ ( key ) except KeyError : return self . __missing__ ( key ) def __missing__ ( self , key ): if not self . default_factory : raise KeyError ( key ) value = self . default_factory () super () . __setitem__ ( key , value ) return value With that built, we can reconstruct WatchedAttribute to hold both \"global\" and \"local\" observers. class WatchedAttribute ( CachingDescriptor , SubjectMixin ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . _local_observers = WeakKeyDefaultDict ( WeakSet ) def __set__ ( self , instance , value ): super () . __set__ ( instance , value ) self . notify ( instance ) def add_observer ( self , observer , instance = None ): if instance is None : super () . add_observer ( observer ) else : self . _local_observers [ instance ] . add ( observer ) def remove_observer ( self , observer , instance = None ): if instance is None : super () . remove_observer ( observer ) else : if observer in self . _local_observers [ instance ]: self . _local_observers [ instance ] . remove ( observer ) def notify ( self , instance ): observers = self . _observers | self . _local_observers [ instance ] for observer in observers : observer . update ( instance ) The real question, now, is how does it handle? It should handle the same as previous iterations on WatchedAttribute except for the specific behavior we've overriden here. I'm also going to add some convience methods to the Person class to make it slightly easier to interact with the observers. class Person : name = WatchedAttribute () def __init__ ( self , name , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . name = name def access_watched ( self , attr ): return getattr ( self . __class__ , attr ) def attach ( self , attr , observer , global_ = False ): watched = self . access_watched ( attr ) inst = None if global_ else self watched . add_observer ( observer , inst ) def detach ( self , attr , observer , global_ = False ): watched = self . access_watched ( attr ) inst = None if global_ else self watched . remove_observer ( observer , inst ) class PrintUpper ( ObserverMixin ): def update ( self , instance ): print ( instance . name . upper ()) pu = PrintUpper () me = Person ( name = None ) me . attach ( 'name' , plnv , global_ = True ) me . attach ( 'name' , pu ) me . name = \"Alec Reiter\" lc rtr ALEC REITER other = Person ( name = \"Ol' Long Johnson\" ) l' lng jhnsn As we can see, the observer that prints the value of Person.name in upper case is bound only to the first instance of Person, where as the one that strips out the vowels and prints that result is bound to all of instances. It's also possible to create an ignore method that would allow specific instances to ignore certain observers as well. Or even better, create a set of rules that can be followed: \"Only invoke this observer if the value doesn't change.\" Something I've curiously ignored is pre-subscribing observers. That is to say, when we create the class we attach a predetermined list of observers to the attribute. This is a feature of the original SubjectMixin class and is inherited to WatchedAttribute (or as Raymond Hettinger would put it: WatchedAttribute delegates the work to SubjectMixin ). class Person : name = WatchedAttribute ( observers = [ plnv ]) def __init__ ( self , name , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . name = name def access_watched ( self , attr ): return getattr ( self . __class__ , attr ) def attach ( self , attr , observer , global_ = False ): watched = self . access_watched ( attr ) inst = None if global_ else self watched . add_observer ( observer , inst ) me = Person ( name = \"Alec Reiter\" ) lc rtr Fin This method of implementing the observer pattern allows a lot of very fine grained control. I'm not advocating it as a good solution — or even a workable solution on its own. There's plenty that's left on the table as far as details and issues go. For example, how would this expand to using a messaging queue (ZeroMQ or Redis) to publish events to? Or how does it interact with asyncio or twisted? Integrating this pattern with an existing framework (blinker for example) would probably be the best solution Rather, it's meant as an introduction to the true power of what you can do with descriptors beyond just making sure a string is all lower case or normalizing floating point numbers to decimal.Decimal instances. Which are valid uses of descriptors, don't take that the wrong way. Some of the concepts introduced here — manipulating descriptors on both the instance and class levels — are used to build tremendously flexible systems. Ever wonder how SQLAlchemy seems to magically treat class attributes as parameters in search queries but then magically they're filled with data on the instance level? Descriptors and that if instance is None check. Further Reading Python 3 Patterns, Recipes and Idioms — Observer Pattern SQLAlchemy ORM Events blinker documentation","title":"Observer Pattern with Descriptors"},{"tags":"tips and tricks","loc":"http://justanr.github.io/code-reuse-in-multiple-forms","text":"Reusing Code, or: How I Learned to Stop Repeating Myself One of the best things about coding is not having to do the same thing over and over again. You automate. You work things into functions and objects and have them worry about completing a series of actions for you. Why wouldn't you do the same thing when actually writing code? There are times where you find yourself repeating code; when this happens, you should consider if it's possible to refactor and break the issue into a reuable piece of code. Generally, the rule of three comes in play: There are two \"rules of three\" in [software] reuse: * It is three times as difficult to build reusable components as single use components, and * a reusable component should be tried out in three different applications before it will be sufficiently general to accept into a reuse library. Facts and Fallacies of Software Engineering #18 Credit to Jeff Atwood's Coding Horror post about the Rule of Three for bringing it to my attention. About This Post This post is just going to be a brief overview of common techniques and patterns to avoid writing the same thing over and over again. Starting with functions and moving into objects, inheritance, mixins, composition, decorators and context managers. There's plenty of other techniques, patterns and idioms that I don't touch on either but this post isn't meant to be an exhaustive list either. Functions Functions are a great way to ensure that a piece of code is always executed the same way. This could be as simple a small expression like (a + b) * x or something that performs a complicated piece of logic. Functions are the most basic form of code reuse in Python. def calc ( a , b , x ): \"\"\"Our business crucial algorithm\"\"\" return ( a + b ) * x calc ( 1 , 2 , 3 ) 9 Python also offers a limited form of anonymous functions called lambda . They're limited to just a single expression with no statements in them. A lot of them time, they serve as basic callbacks or as key functions for a sort or group method. The syntax is simple and the return value is the outcome of the expression. sorted ([( 1 , 2 ), ( 3 , - 1 ), ( 0 , 0 )], key = lambda x : x [ 1 ]) [(3, -1), (0, 0), (1, 2)] While lambdas are incredibly useful in many instances, it's generally considered bad form to assign them to variables (since they're supposed to be anonymous functions), not that I've never done that when it suited my needs. ;) Objects Objects are really the poster child for code reuse. Essentially, an object is a collection of data and functions that inter relate. Many in the Python community are fond of calling them a pile of dictionaries — because that's what they essentially are in Python. Objects offer all sorts of possibilities such as inheritance and composition, which I'll briefly touch upon here. For now, a simple example will suffice: take our business critical algorithm and turn it into a spreadsheet row class SpreadsheetRow : def __init__ ( self , a , b , x ): self . a = a self . b = b self . x = x def calc ( self ): return calc ( self . a , self . b , self . x ) row = SpreadsheetRow ( 1 , 2 , 3 ) print ( row . calc ()) 9 Notice how we're already reusing code to find our business critical total of 9! If later, someone in accounting realizes that we should actually be doing a * (b + x) , we simply change the original calculation function. Inheritance Inheritance is simply a way of giving access of all the data and methods of a class to another class. It's commonly called \"specialization,\" though Raymond Hettinger aptly describes it as \"delegating work.\" If later, accounting wants to be able to label all of our spreadsheet rows, we could go back and modify the original class or we could design a new one that does this for us. Accessing information in the inherited class is done through super() , I won't delve into it's details here but it is quite super . class LabeledSpreadsheetRow ( SpreadsheetRow ): def __init__ ( self , label , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . label = label row = LabeledSpreadsheetRow ( label = '1' , a = 1 , b = 2 , x = 3 ) print ( \"The total for {} is {}\" . format ( row . label , row . calc ())) The total for 1 is 9 Mixins Mixins are a type of multiple inheritance, which I won't fully delve into here because it's a complicated and touchy subject. However, Python supports it. Because of this and it's support for duck typing, we can completely forego the use of Interfaces and Traits which are common in single inheritance languages. Mixins are a way of writing logic that is common to many objects and placing it in a single location. Mixins are also classes that aren't meant to be instantiated on their own either, since they represent a small piece of a puzzle rather than the whole picture. A common problem I use mixins for is creating a generic __repr__ method for objects. class ReprMixin : def __repr__ ( self ): name = self . __class__ . __name__ attrs = ', ' . join ([ \"{}={}\" . format ( k , v ) for k , v in vars ( self ) . items ()]) return \"<{} {}>\" . format ( name , attrs ) class Row ( LabeledSpreadsheetRow , ReprMixin ): pass row = Row ( label = '1' , a = 1 , b = 2 , x = 3 ) repr ( row ) '<Row b=2, x=3, a=1, label=1>' This showcases the power of inheritance and mixins: composing complex objects from smaller parts into what you're wanting. The actual class we're using implements no logic of it's own but we're now provided with: A repr method A calculation method A label attribute Data points to calculate Composition Composition is a fancy way of saying we're going to build an object using other objects, in other words: composing them from parts. It's a similar idea to inheritance, but instead the objects we're using are stored as attributes on the main object. We have spreadsheet rows, why not a spreadsheet to hold them? class Spreadsheet ( ReprMixin ): def __init__ ( self , name ): self . name = name self . rows = [] def show_all ( self ): for row in self . rows : print ( \"The total for {} is {}\" . format ( row . label , row . calc ())) def total ( self ): return sum ( r . calc () for r in self . rows ) sheet = Spreadsheet ( \"alec's totals\" ) sheet . rows . extend ([ Row ( label = 1 , a = 1 , b = 2 , x = 3 ), Row ( label = 2 , a = 3 , b = 5 , x = 8 )]) sheet . show_all () print ( sheet . total ()) repr ( sheet ) The total for 1 is 9 The total for 2 is 64 73 \"<Spreadsheet name=alec's totals, rows=[<Row b=2, x=3, a=1, label=1>, <Row b=5, x=8, a=3, label=2>]>\" Here we're not only reusing the ReprMixin so we can have accurate information about our Spreadsheet object, we're also reusing the Row objects to provide that logic for free, leaving us to just implement the show_all and total methods. Decorators Decorators are a way factoring logic out of a class or function and into another class or function. Or to add extra logic to it. That sounds confusing, but it's really not. I've written about them elsewhere , so if you're unfamiliar with them I recommend reading that first. Here, we're going to use two decorators Python provides in the standard library called total_ordering so we can sort our Row objects and the other is the property decorator which allows us to retreat a function as if it were an attribute (via the descriptor protocol which is a fantastic code reuse ability that I won't explore here). from functools import total_ordering @total_ordering class ComparableRow ( Row ): @property def __key ( self ): return ( self . a , self . b , self . x ) def __eq__ ( self , other ): return self . __key == other . __key def __lt__ ( self , other ): return self . __key < other . __key rows = sorted ([ ComparableRow ( label = 1 , a = 3 , b = 5 , x = 8 ), ComparableRow ( label = 2 , a = 1 , b = 2 , x = 3 )]) print ( rows ) [<ComparableRow b=2, x=3, a=1, label=2>, <ComparableRow b=5, x=8, a=3, label=1>] What total_ordering does is provide all the missing rich comparison operators for us. Meaning even though we only defined __lt__ and __eq__ here, we also have __le__ , __gt__ , __ge__ , and __ne__ available to us. Decorators are an incredibly powerful to modify your regular Python functions and objects. Context Managers Context managers are a way of handling operations you typically do in pairs: open a file, close a file; start a timer, end a timer; acquire a lock, release a lock; start a transactio, end a transaction. Really, anything you do in pairs should be a candidate for context managers. Writing context managers is pretty easy, depending on which method you go about. I'll likely explore them in a future post. For now, I'm going to stick to using the generator context manager form as an example: from contextlib import contextmanager @contextmanager def greeting ( name = None ): print ( \"Before the greeting.\" ) yield \"Hello {!s}\" . format ( name ) print ( \"After the greeting.\" ) with greeting ( \"Alec\" ) as greet : print ( greet ) Before the greeting. Hello Alec After the greeting. We won't be writing a context manager here, but rather using one to implement an \"alternate constructor\" for our Spreadsheet class. Alternate constructors are a way of initializing an object in a specific way. These are especially handy if you find yourself occasionally creating an object under certain conditions. Consider dict.fromkeys which lets you fill a dictionary with keys from an iterable that all have the same value: print ( dict . fromkeys ( range ( 5 ), None )) {0: None, 1: None, 2: None, 3: None, 4: None} In our case, we'll probably want to draw our information from a CSV file occasionally. If we do it often enough, writing the setup logic could become tedious to rewrite all over the place. import csv class CSVSpreadsheet ( Spreadsheet ): @classmethod def from_csv ( cls , sheetname , filename ): sheet = cls ( sheetname ) with open ( filename ) as fh : reader = csv . reader ( fh . readlines ()) sheet . rows = [ ComparableRow ( * map ( int , row )) for row in reader ] return sheet sheet = CSVSpreadsheet . from_csv ( 'awesome' , 'row.csv' ) sheet . show_all () The total for 1 is 9 The total for 2 is 64 The total for 3 is 16 Fin Hopefully this gives you an idea for reusing code in your own projects. Maybe you'll write your own crappy spreadsheet object as well.","title":"Code Reuse in Multiple Forms"},{"tags":"tutorials","loc":"http://justanr.github.io/describing-descriptors","text":"An Aside: Just like my post on decorators, I've decided to rewrite this post as well because it suffered from the same issue: \"Look at all this code…and hey, there's explainations as well.\" Instead of exploring patterns, like I did in the decorator post, I'm going to focus in on one example use that explores several aspects of descriptors all at once. Of course, I'll step through it piece by piece. There's actually going to be two major sections to this post: Python Object Attribute Access Writing Our First Descriptor Also, this post was built with Python 3.4 in mind. While it's foolish to think that everyone everywhere is using the latest and greatest Python release, it's what I've been using primarily lately. That's me. Updated Nov. 8th, 2014: Added concrete examples of behind the scenes action of descriptors as well as a brief explaination of what __delete__ does. Controlling Attribute Access Before digging into descriptors, it's important to talk about attribute access. Because at the end of the day, that's what descriptors do for us. There's really two ways of going about this with explicitly building our own descriptor. Getters and Setters This is what you'll see in many languages: explicit getters and setters. They're methods that handle attributes for us. This is very common in Java and PHP (or at least as of the last time I seriously used PHP ). Essentially, the idea is to always expect to interact with a method instead of an attribute itself. There's nothing wrong with this if it's what your language of choice supports and you need to control access. # it's a contrived example # but bear with me here # pretend this is *important business logic* def to_lower ( value ): return value . lower () class Person : def __init__ ( self , name ): self . __name = None self . set_name ( name ) def get_name ( self ): return self . __name def set_name ( self , name ): self . __name = to_lower ( name ) monty = Person ( name = \"John Cleese\" ) print ( monty . get_name ()) monty . set_name ( \"Eric Idle\" ) print ( monty . get_name ()) john cleese eric idle That's fine and dandy. If that's what you language supports. Python, in my opinion, handles this better. @property The real way you'd write this in Python is by using @property . Which is a decorator, which we are familiar with. I won't go into details about what's going on behind the scenes yet. class Person : def __init__ ( self , name ): self . __name = None self . name = name @property def name ( self ): return self . __name @name.setter def name ( self , name ): self . __name = to_lower ( name ) monty = Person ( name = \"Graham Chapman\" ) print ( monty . name ) monty . name = \"Terry Gilliam\" print ( monty . name ) graham chapman terry gilliam For now, don't worry that I have two methods called name . It'll become apparent in a little bit. That is a much cleaner interface to the class. As far as the calling code is concerned, name is just another attribute. This is fantastic if you designed an object and later realized that you need to control how an attribute is returned or set (or deleted, but I'm not going to delve into that aspect of descriptors at all in this post). To many people, this is where they'd stop with controlling attribute access. And frankly, I don't really blame them. @property seems to be magic enough behind the scenes already. Why is it disappearing into name , where does setter come from, how does it work? These are questions that might go unasked or, worse: unanswered. @property suffices in most situations, especially when you only need to control one attribute in a specific way. But imagine if we had to ensure two or three attributes were all lower case? You might be tempted to replicate the code all the way down. You don't mind a little repitition, do you? class Person : def __init__ ( self , email , firstname , lastname ): self . __f_name = None self . __l_name = None self . __email = None self . f_name = firstname self . l_name = lastname self . email = email @property def f_name ( self ): return self . __f_name @f_name.setter def f_name ( self , value ): self . __f_name = to_lower ( value ) @property def l_name ( self ): return self . __l_name @l_name.setter def l_name ( self , value ): self . __l_name = to_lower ( value ) @property def email ( self ): return self . __email @email.setter def email ( self , value ): self . __email = to_lower ( value ) monty = Person ( firstname = 'Michael' , lastname = 'Palin' , email = 'MichaelPalin@montypython.com' ) print ( monty . f_name , monty . l_name , monty . email ) michael palin michaelpalin@montypython.com Like I said, it's a contrived example. But instead of ensuring things are lower cased, imagine you're attempting to keep text fields in a GUI synchronized with the state of an object or you're working with a database ORM . Things will very quickly get out of hand if you have to property a ton of stuff with the logic repeated except for the names. Behind the Scenes I'm not going to 100% faithfully recreate @property here, frankly I don't see a point. But I do want to dissect it from what we can observe on the surface. property is a decorator. As a decorator it's a function that takes a function and returns a callable. This we know. The callable created by property is an object The object has at least one method on it, setter , that somehow controls how a variable is set However, if we inspect property (my preferred way is with IPython's ? and ?? magics) we learn that there is one more method and three attributes that aren't immediately obvious to us. deleter is the missing method, which handles how an attribute is deleted with del fget , fset and fdel are the attributes, which are unsurprisingly the original functions for getting, setting and deleting attributes. For the above example, fget and fset are our two name methods above. They actually get hidden away into an object decorator, which is how we have two methods with the same name without worry. Data model This about as far as we can get without understanding how Python accesses attributes on objects. I won't attempt to give a complete in depth analysis of how Python actually accesses and sets attributes on objects, but this is a simplified, high level view of what's going on (ignoring the existence of __slots__ , which actually replaces the underlying __dict__ with a set of descriptors and what's going on there I'm not 100% sure of). Call __getattribute__ Look up in the object's dictionary Look up in the class's dictionary Walk the MRO and look up in those dictionaries If present and all other look ups have failed, call __getattr__ If the name resolves to a descriptor, return the value of it's __get__ method If all all else fails, raise an AttributeError That sixth point is the most pertinent to us. An object that defines a __get__ method is known as a descriptor. property is actually an object that does this. In effect, it's __get__ method resembles this: def __get__ ( self , instance , type = None ): return self . fget ( instance ) Similarly, the resolution for setting an attribute looks like this: If present, call __setattr__ If the name resolves to a descriptor, call it's __set__ method Stuff the value into the object's dict So, property's __set__ looks like this: def __set__ ( self , instance , value ): self . fset ( instance , value ) I'm glossing over raising attribute errors for attributes that don't support reading or writing, but property does that. There's two reasons I'm positive this is how these two methods look is because I'm familar with the descriptor protocol and Raymond Hettinger wrote about it here . Descriptor Protocol There's three parts to the descriptor protocol: __get__ , __set__ and __delete__ . Like I said, I'm not delving into deleting attributes here, so we'll remain unconcerned with that. But any object that defines at least one of these methods is a descriptor. The best way to dissect a descriptor is provide an example implementation. This example isn't actually going to do anything except emulate regular attribute access. class Descriptor : def __init__ ( self , name ): self . name = name def __get__ ( self , instance , cls ): return instance . __dict__ . get ( self . name , None ) def __set__ ( self , instance , value ): instance . __dict__ [ self . name ] = value def __delete__ ( self , instance ): del instance . __dict__ [ self . name ] class Thing : frob = Descriptor ( name = 'frob' ) def __init__ ( self , frob ): self . frob = frob t = Thing ( frob = 4 ) print ( t . frob ) 4 get def __get__(self, instance, type): self is the instance of the descriptor itself, just like any other object. instance is an instance of the class it's attached to type is the actual object that it's attached to, I typically prefer to use cls as the name here because it's slightly more clear to me. owner is another common name, but slightly more confusing to me. Above, when we request t.frob what's actually happening behind the scenes is Python is calling Thing.frob.__get__(t, Thing) instead of passing Thing.frob.__get__ directly to the print function. The reason the actual class is passed as well is \"to give you information about what object the descriptor is part of\", to quote Chris Beaumont . While I've not made use of inspecting which class the descriptor is part of, this could be valuable information. You could also call Thing.frob.__get__(t, Thing) explicitly if you'd like, but Python's data model will handle this for us. set def __set__(self, instance, value): self again no surprises here, this is the instance of the descriptor instance this is an instance of the class it's attached to value is the value you're passing in, if you've used property before, there's no surprise here. Again, what's happening behind the scenes when we set t.frob to something (in this case, just in Thing.__init__ ), Python passes information to Thing.frob.__set__ , the information just being the instance of Thing and the value we're setting. delete def __delete__(self, instance): No surprises here. And despite that I said I wasn't going to go into deleting attributes with descriptors, I've included it for completion's sake. The delete method handles what happens when we call del t.frob . Data vs Non-Data Descriptor This is something you're going to encounter when reading about and working with descriptors: the difference between a data and non-data descriptor and how Python treats both when looking up an attribute. Data Descriptor A data descriptor is a descriptor that defines both a __get__ and __set__ method. These descriptors recieve higher priority if Python finds a descriptor and a __dict__ entry for the attribute being looked up. Already, you can see that attribute access isn't as clear cut as we thought it was. Non-Data Descriptor A non-data descriptor is a descriptor that defines only a __get__ method. These descriptors recieve a lower priority if Python finds both the descriptor and a __dict__ entry. What does this mean? By using descriptors we can create reusable properties , as Chris Beaumont calls them and I find to be an incredibly apt definition. But there's quite a few pits we can fall into. For the rest of this post, I'm going to focus on rebuilding our lower case properties as a reusable descriptor. In another post, I'm going to more fully explore some of the power these put at our finger tips. Our First Descriptor So far, we know a descriptor needs to define at least one of __get__ , __set__ , or __delete__ . Let's try our hand at building a LowerString descriptor. class LowerString : def __init__ ( self , value = None ): self . value = value def __get__ ( self , instance , cls ): return self . value def __set__ ( self , instance , value ): self . value = to_lower ( value ) class Person : f_name = LowerString () l_name = LowerString () email = LowerString () def __init__ ( self , firstname , lastname , email ): self . f_name = firstname self . l_name = lastname self . email = email monty = Person ( firstname = \"Terry\" , lastname = \"Jones\" , email = \"TerryJONES@montyPython.com\" ) print ( monty . f_name , monty . l_name , monty . email ) terry jones terryjones@montypython.com While this isn't as perfectly clean as we might like, it's certainly a lot prettier than using a series of property decorators and way nicer than defining explicit getters and setters. However, there's a big issue here. If you can't spot it, I'll point it out. me = Person ( firstname = \"Alec\" , lastname = \"Reiter\" , email = \"alecreiter@fake.com\" ) print ( me . f_name , me . l_name , me . email ) print ( monty . f_name , monty . l_name , monty . email ) alec reiter alecreiter@fake.com alec reiter alecreiter@fake.com …oh. Well that happened. And the reason for this, and this what tripped me up when I began reading about descriptors, is that each instance of person shares the same instances of LowerString for the three properties. Descriptors enforce a shared state by virture of being instances attached to a class rather than instances . So instead of composing an instance of an object with other objects (say a Person object composed of Job , Nationality and Gender instances), we compose a class out of object instances. If we examine the __dict__ for both the class and the instance, it becomes apparent where Python finds these values at: print ( Person . __dict__ ) print ( monty . __dict__ ) {'__doc__': None, '__weakref__': <attribute '__weakref__' of 'Person' objects>, '__init__': <function Person.__init__ at 0x7f337e2e58c8>, 'f_name': <__main__.LowerString object at 0x7f337ce4a5c0>, 'l_name': <__main__.LowerString object at 0x7f337ce4a550>, 'email': <__main__.LowerString object at 0x7f337ce4a630>, '__module__': '__main__', '__dict__': <attribute '__dict__' of 'Person' objects>} {} Since the descriptors aren't attached at the instance level, Python moves up to the class level where it finds the attribute we're requesting, sees it's a descriptor and then calls the __get__ method. If you attempt to attach these descriptors at the instance level instead, you end up with this: class Person : def __init__ ( self , firstname , lastname , email ): self . f_name = LowerString ( firstname ) self . l_name = LowerString ( lastname ) self . email = LowerString ( email ) me = Person ( firstname = \"Alec\" , lastname = \"Reiter\" , email = \"alecreiter@fake.com\" ) print ( me . f_name , me . l_name , me . email ) <__main__.LowerString object at 0x7f337ce4e390> <__main__.LowerString object at 0x7f337ce4e400> <__main__.LowerString object at 0x7f337ce4e438> So explicitly attaching them to the instances won't work. But remember, Python passes the instance for us automatically. Let's try storing the value on the underlying object by accessing it's __dict__ attribute: class LowerString : def __init__ ( self , label ): self . label = label def __get__ ( self , instance , cls ): return instance . __dict__ [ self . label ] def __set__ ( self , instance , value ): instance . __dict__ [ self . label ] = to_lower ( value ) class Person : f_name = LowerString ( 'f_name' ) l_name = LowerString ( 'l_name' ) email = LowerString ( 'email' ) def __init__ ( self , firstname , lastname , email ): self . f_name = firstname self . l_name = lastname self . email = email monty = Person ( firstname = \"Carol\" , lastname = \"Cleaveland\" , email = \"seventh@montypython.com\" ) print ( monty . f_name , monty . l_name , monty . email ) carol cleaveland seventh@montypython.com And surely this works, but we've run into the issue of repeating ourselves again. It'd be nice if we could simply do something to automatically fill in the label for us. David Beazley addressed this problem in the 3rd Edition of the Python Cookbook . class checkedmeta ( type ): def __new__ ( cls , clsname , bases , methods ): # Attach attribute names to the descriptors for key , value in methods . items (): if isinstance ( value , Descriptor ): value . name = key return type . __new__ ( cls , clsname , bases , methods ) Of course this means, we need to make two small changes to our descriptor: changing label to name and inheriting from a base Descriptor class. class Descriptor : def __init__ ( self , name = None ): self . name = name class LowerString ( Descriptor ): def __get__ ( self , instance , cls = None ): return instance . __dict__ [ self . name ] def __set__ ( self , instance , value ): instance . __dict__ [ self . name ] = to_lower ( value ) class Person ( metaclass = checkedmeta ): f_name = LowerString () l_name = LowerString () email = LowerString () def __init__ ( self , firstname , lastname , email ): self . f_name = firstname self . l_name = lastname self . email = email monty = Person ( firstname = \"Carol\" , lastname = \"Cleaveland\" , email = \"seventh@montypython.com\" ) print ( monty . f_name , monty . l_name , monty . email ) carol cleaveland seventh@montypython.com And this is very nice and handy. If later, we wanted to create an EmailValidator descriptor, so long as we adhere to the pattern laid out here, we can attach them to any class that uses the checkedmeta metaclass and it'll behave as expected. But there's something still very annoying going on and it's one of the biggest gripes with property is that a getter has to be defined even if I'm only interested in the setter . If you set fget to None, you end up getting an attribute error that says it's write only. If we examine our current implementation, we'll notice something else as well: print ( Person . __dict__ ) print ( monty . __dict__ ) {'__doc__': None, '__weakref__': <attribute '__weakref__' of 'Person' objects>, '__init__': <function Person.__init__ at 0x7f337ce38c80>, 'f_name': <__main__.LowerString object at 0x7f337ce31588>, 'l_name': <__main__.LowerString object at 0x7f337ce31550>, 'email': <__main__.LowerString object at 0x7f337ce316a0>, '__module__': '__main__', '__dict__': <attribute '__dict__' of 'Person' objects>} {'email': 'seventh@montypython.com ', 'f_name': 'carol', 'l_name': 'cleaveland'} There's now the descriptors living at the class level and the values living at the instance level. Let's add some \"debugging\" print calls to see what's happening on the inside. class LowerString ( Descriptor ): def __init__ ( self , name = None ): self . name = name def __get__ ( self , instance , cls = None ): print ( \"Calling LowerString.__get__\" ) return instance . __dict__ [ self . name ] def __set__ ( self , instance , value ): print ( \"Calling LowerString.__set__\" ) instance . __dict__ [ self . name ] = to_lower ( value ) class Person ( metaclass = checkedmeta ): f_name = LowerString () l_name = LowerString () email = LowerString () def __init__ ( self , firstname , lastname , email ): self . f_name = firstname self . l_name = lastname self . email = email monty = Person ( firstname = \"Carol\" , lastname = \"Cleaveland\" , email = \"seventh@montypython.com\" ) print ( monty . f_name , monty . l_name , monty . email ) Calling LowerString.__set__ Calling LowerString.__set__ Calling LowerString.__set__ Calling LowerString.__get__ Calling LowerString.__get__ Calling LowerString.__get__ carol cleaveland seventh@montypython.com Python gives special preference to data descriptors (as described before). However, we can remove this special preference by simply removing the __get__ method. Arguably, this is the most useless part of this descriptor anyways, it's not transforming the result or providing a lazy calculation, it's simply aping what Person.__getattribute__ would do in the first place: Find the value in the object's dictionary. If we remove this, then we're left with only a setter, which is what we really wanted in the first place: class LowerString ( Descriptor ): def __init__ ( self , name = None ): self . name = name def __set__ ( self , instance , value ): print ( \"Calling LowerString.__set__\" ) instance . __dict__ [ self . name ] = to_lower ( value ) class Person ( metaclass = checkedmeta ): f_name = LowerString () l_name = LowerString () email = LowerString () def __init__ ( self , firstname , lastname , email ): self . f_name = firstname self . l_name = lastname self . email = email monty = Person ( firstname = \"Carol\" , lastname = \"Cleaveland\" , email = \"seventh@montypython.com\" ) print ( monty . f_name , monty . l_name , monty . email ) monty . f_name = \"Cheryl\" print ( monty . f_name ) Calling LowerString.__set__ Calling LowerString.__set__ Calling LowerString.__set__ carol cleaveland seventh@montypython.com Calling LowerString.__set__ cheryl And this has to do with how Python sets attributes, which examined above. Again, it's giving special precedence to the descriptor, which is what we want in the first place. However, when we access the attribute, it sees there's an entry in the object's __dict__ and the class's __dict__ but the latter doesn't have a __get__ method, which causes it to default back to the object's entry. print ( Person . __dict__ ) print ( monty . __dict__ ) {'__doc__': None, '__weakref__': <attribute '__weakref__' of 'Person' objects>, '__init__': <function Person.__init__ at 0x7f337ce43730>, 'f_name': <__main__.LowerString object at 0x7f337ce55710>, 'l_name': <__main__.LowerString object at 0x7f337ce55748>, 'email': <__main__.LowerString object at 0x7f337ce55780>, '__module__': '__main__', '__dict__': <attribute '__dict__' of 'Person' objects>} {'email': 'seventh@montypython.com ', 'f_name': 'cheryl', 'l_name': 'cleaveland'} Leaving us with just the setter and no unnecessary data or method duplication. Going forward In the original post, I also explored building an oberserver pattern with descriptors, something Chris Beaumont also touches upon briefly but leaves a lot on the table as far registering callbacks on every instance and specific instances of classes. I plan on touching on this again in a future post. But for now, I'm hoping this leaves a much better impression of descriptors than my original post. Again, this isn't meant to be a tell all about descriptors but hopefully serves to clarify a lot of the magic that appears to happen behind the scenes when you're using SQLAlchemy and defining models. Further Learning Chris Beaumont's Python Descriptors Demystified David Beazley's Python 3 Metaprograming this presentation explores far more than just descriptors and delves into a lot of advanced concepts Luciano Ramalho's Encapsulation with Descriptors Python.org Descriptor How To by Raymond Hettinger And whole host of SO Questions, just to show a few: [How does the @property decorator work?]( http://stackoverflow.com/questions/17330160/how-does-the-property-decorator-work/ ) Understanding get and set and Python descriptors.","title":"Describing Descriptors"},{"tags":"tutorial","loc":"http://justanr.github.io/python-and-audio-metadata","text":"Metadata and You You're probably familiar, at least in passing, with the concept of metadata. It's data about data. In terms of audio files, it's things like: Bitrate Duration/Length Artist Album Track Position (if available) Year released (if available) Today, I'm using TinyTag ( pip install tinytag ) to extract metadata (and TinyTag only supports this), but there's a whole plethora of libraries to extract and set audio metadata: Mutagen docs source eyed3 docs source stagger source Aside I actually don't spend a terrible amount of time on actually pulling the metadata out. It's extremely straightforward with TinyTag. I spend more time with processing and massaging that data into information we can really work with. I revisit the cacheing decorator from the decorator post (and don't follow much of my own advice — it's kind of a do as I say, not as I do sort of thing, even then I'm not even really giving that great of advice) as well as improving on the Artist/Album/Track mock data models from the API post as well. There's also a little bit about recursively walking directories and selectively looking for the files you want. Becoming the NSA of your Music Getting the metadata is incredibly easy. I'm only going to extract it from one artist today, but extending this out to covering your whole library is easy (well, as long as you store your music in one location and in a reasonable pattern). from tinytag import TinyTag import os , os.path as path # audio types TinyTag supports valid_types = ( 'mp3' , 'ogg' , 'oga' , 'wav' , 'flac' ) # artist directory acid_bath = '/home/justanr/Music/Acid Bath/' #extract the directories within the artist directory albums = [ path . join ( acid_bath , a ) for a in os . listdir ( acid_bath ) if not 'Demos' in a ] tracks = [] # extract tracks from each album directory for album in albums : _tracks = [] for t in os . listdir ( album ): # did you know it was possible to pass an iterable to str.endswith? if not t . endswith ( valid_types ): continue _tracks . append ( path . join ( album , t )) tracks . extend ( _tracks ) del _tracks # no need for hanger-ons. # convert track paths to TinyTag objects tracks = [ TinyTag . get ( t ) for t in tracks ] # string representation is actually a # string representation of a dictionary # created on the fly from the public # values on the actual object print ( tracks [ 0 ]) {'year': '1994', 'duration': 297.87708785043435, 'track': 'x035', 'audio_offset': 192239, 'artist': 'x03Acid Bath', 'filesize': 11915076, 'bitrate': 320.0, 'title': 'x03Jezebel', 'album': 'x03When the Kite String Pops', 'track_total': '14', 'samplerate': 44100} As an aside, if you've never listened to Acid Bath , I can't recommend them enough. Hard rocking, groovy sludgey metal with lyrics that are equal parts Lewis Carroll, grotesque and drug binge. Some songs are more the typical what the layman thinks of metal (Jezebel, Cheap Vodka) but with songs like Venus Blue, Bleed Me an Ocean and Dead Girl there's really something for most people. Data Munging and You Alright, looks like I have an encoding issue. There is something to note here though: \\x03 denotes END OF TEXT and to my understanding you're suppose to see it in ID3 tags (though, that's based on second hand information as my search-fu didn't turn up anything). However, I have actually run into an encoding issue using TinyTag (I'm not sure if the problem is with the file, Python or TinyTag). An example: nzp = TinyTag . get ( '/home/justanr/Music/At the Drive-In/Relationship of Command/11 Non‐Zero Possibility.mp3' ) print ( 'Expected: At the Drive-In - Relationship of Command - 11 - Non-Zero Possibility' ) print ( 'Actual: ' , ' - ' . join ([ nzp . artist , nzp . album , nzp . track , nzp . title ])) # This is just a bandage. # The actual issue is an encoding issue # I've not been able to track down. # I'd like to thank /u/anossov and /u/ryeguy146 # for help, even though we all agree this isn't # the correct way to address the issue at hand def fixer ( value , ignore = ( AttributeError , UnicodeEncodeError ), handle = None ): '''Actual fixer function for fix_track ignore is a tuple of exceptions we should just discard. handle is an optional exception handler. ''' try : value = value . encode ( 'latin-1' ) . decode ( 'utf-8' ) . strip () # matching \\x03 is frustrating # again, just a crutch to lean on if not value [ 0 ] . isprintable (): value = value [ 1 :] except ignore as e : if handle : handle ( e ) else : pass finally : # we always end up here return value def fix_track ( track , fixer = fixer , fields = ( 'artist' , 'album' , 'title' , 'track' , 'year' , 'track_total' ), int_convert = ( 'track' , 'year' , 'track_total' ) ): '''Fix encoding issue encountered on some tracks. Accepts a track object and attempts to massage the data in our favor. * fixer is the function we want to run on this track to correct data * fields in the specific fields we'd like to attempt to correct * int_convert is a subset of fields that is data that should be integers ''' for f in fields : value = getattr ( track , f ) if not value : # value is likely None # we'll pass on this value # to avoid blowing up continue else : value = fixer ( value ) if f in int_convert : try : value = int ( value ) except ValueError : pass setattr ( track , f , value ) # TODO: need to make this mutable # for now, it's hardcoded as TinyTag # stores duration as a float track . duration = int ( track . duration ) # returning the track allows us # to be flexible in application # of this function return track nzp = fix_track ( nzp ) print ( 'After Fixing: ' , ' - ' . join ([ nzp . artist , nzp . album , str ( nzp . track ), nzp . title ])) Expected: At the Drive-In - Relationship of Command - 11 - Non-Zero Possibility Actual: \u0003At the DriveâIn - \u0003Relationship of Command - \u000311 - \u0003NonâZero Possibility After Fixing: At the Drive‐In - Relationship of Command - 11 - Non‐Zero Possibility As you can see, encoding issues are painful . This bandage is the best I can come up with until I can actually track down and address the issue. However, this works for now (and like so many # TODO : Address actual issue functions, it'll inevitable end up in my codebase permanently). This is an example of data munging . You'll mostly hear that phrase in machine learning and data processing. But when you think about, isn't all programming data processing? Data munging is the process of taking in data and massaging it into a form you can work with. Most of the time, this involves just converting data from one type to another. Sometimes it involves just throwing data away and knowing when you should. In this case, it's taking a TinyTag object and breaking it into three separate pieces: Artist, Album and Track data models. Just like in the API data mocking post, these stand in for actual data models. However, now, there's a fair bit of magic going on: cacheing, auto registration of albums and tracks on relevant models, and for good measure: total ordering. Cacheing is not done entirely correctly, but do note the use of WeakValueDictionary which stores weak references. Weak references allow an object to be garbage collected when it's the only remaining reference left. Meaning if we created a bunch of Artist objects, stuffed them into permanent storage somewhere and then deleted the objects (and all their \"strong\" references), Python can come clean up some memory for us. Which is incredibly useful when dealing with a lot of data at once. import itertools as it from functools import wraps , total_ordering from weakref import WeakValueDictionary # Too lazy to spend time manipulating # __new__ or making a metaclass # (actually, I spent a lot of time being frustrated at them) # to handle this so there's this hack def cache ( cls ): '''Helper caching and identification methods.''' _registry = WeakValueDictionary () _ids = it . count ( 1 ) def by_name ( name ): return _registry . get ( name , None ) @wraps ( cls ) def wrapper ( name , * args , ** kwargs ): nonlocal _registry , _ids if name not in _registry : # create a new instance of the class instance = cls ( name = name , * args , ** kwargs ) # generate id for the instance instance . id = next ( _ids ) # register the instance _registry [ name ] = instance #reuturn the instance return _registry [ name ] wrapper . by_name = by_name return wrapper @total_ordering class ComparableMixin ( object ): def _compare ( self , other , method ): try : return method ( self . _cmpkey (), other . _cmpkey ()) except ( AttributeError , TypeError ): # _cmpkey not implemented, or return different type, # so I can't compare with \"other\". return NotImplemented def __hash__ ( self ): return hash ( self . _cmpkey ()) def __lt__ ( self , other ): return self . _compare ( other , lambda s , o : s < o ) def __eq__ ( self , other ): return self . _compare ( other , lambda s , o : s == o ) @cache class Artist ( ComparableMixin ): def __init__ ( self , name , albums = None ): self . id = None self . name = name # albums should be a list of albums # but it becomes a dictionary # probably shouldn't actually do this self . albums = albums or {} if self . albums : albums = [ Album . by_name ( a ) for a in albums ] self . albums = { a . name : a for a in albums if a is not None } def _cmpkey ( self ): '''This key is used by ComparableMixin for ordering. It is also used for hashing. Including the id is good practice here to ensure that we don't accidentally create two objects with the same name that makes this useless in a hashtable. ''' # tuples compare across like this: # self[0] op other[0] ... self[n] op other[n] # until the tuples don't match # then Python sorts them accordingly # meaning often the whole tuple doesn't get # compared to another tuple # this is helpful when you want a default # multivalue sort return ( self . name , self . id ) def __repr__ ( self ): return \"<Artist name={} id={}>\" . format ( self . name , self . id ) @cache class Album ( ComparableMixin ): def __init__ ( self , name , artist = None , tracks = None ): self . id = None self . name = name self . artist = Artist . by_name ( artist ) self . tracks = tracks or [] if self . artist : self . artist . albums [ self . name ] = self if tracks : tracks = [ Track . by_name ( t ) for t in tracks ] self . tracks = [ t for t in tracks if not t is None ] def _cmpkey ( self ): return ( self . artist , self . name , self . id ) def __repr__ ( self ): artist = self . artist . name if self . artist else \"<BLANK>\" return \"<Album name={} artist={} id={}>\" . format ( self . name , artist , self . id ) @cache class Track ( ComparableMixin ): def __init__ ( self , name , track , length , artist = None , album = None ): self . id = None self . name = name self . track = track self . length = length # in seconds self . artist = Artist . by_name ( artist ) self . album = Album . by_name ( album ) if self . album : self . album . tracks . append ( self ) def _cmpkey ( self ): return ( self . album , self . track , self . name , self . id ) def __repr__ ( self ): album = self . album . name if self . album else \"<BLANK>\" artist = self . artist . name if self . artist else \"<BLANK>\" return \"<Track artist={} album={} track={} name={} id={}>\" . format ( artist , album , self . track , self . name , self . id ) # Mini unit tests ;) # gorguts = Artist(name='Gorguts') # obscura = Album(name='Obscura', artist='Gorguts') # obscura_track = Track(name=\"Obscura\", track=1, length=244, artist=\"Gorguts\", album=\"Obscura\") # assert obscura.artist is gorguts # assert obscura.name in gorguts.albums # assert obscura_track in obscura.tracks # assert obscura == gorguts.albums['Obscura'] # assert Artist.by_name('blank') is None # assert Artist(name='test').id == 2 # assert Artist(name='Gorguts') is gorguts # print(gorguts, obscura, obscura_track) So much hard work taken care of for us with a little magic. The little unittest at the end is to assure us everything is working as intended. Uncomment them to see them in action. The Comparable mixin, I shamelessly stole , but it'll make life much easier on the other end of this conversion if we need to sort a collection of Artist , Album or Track objects. I added the __hash__ method to it. Technically it should be a HashableMixin , there's already a ton of stuff going on in this frame, I decided another class would just add to the noise. Now, we need to massage our list of Acid Bath tracks into these models. You didn't think I forgot about them, did you? from collections import namedtuple tracks = [ fix_track ( track ) for track in tracks ] # our sort and groupby key function key = namedtuple ( 'key' , [ 'artist' , 'album' , 'track' ]) compare = lambda t : key ( t . artist , t . album , t . track ) # named after the pattern adaptor = lambda t : { 'artist' : t . artist , 'album' : t . album , 'length' : t . duration , 'name' : t . title , 'track' : t . track } tracks . sort ( key = compare ) # this keeps us from creating a big list # with the same artist, albums and tracks # over and over by accident artists = set () albums = set () _tracks = set () for keys , grouped in it . groupby ( tracks , key = compare ): artists . add ( Artist ( name = keys . artist )) albums . add ( Album ( name = keys . album , artist = keys . artist )) for track in grouped : _tracks . add ( Track ( ** adaptor ( track ))) artists = list ( artists ) albums = list ( albums ) tracks = list ( _tracks ) # no hanger ons! del _tracks print ( \"Artists: \" , * artists , sep = ' \\n ' ) print ( \" \\n Albums: \" , * sorted ( albums ), sep = ' \\n ' ) print ( \" \\n Tracks: \" , * sorted ( tracks ), sep = ' \\n ' ) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-9-2ec183945278> in <module>() 1 from collections import namedtuple 2 ----> 3 tracks = [fix_track(track) for track in tracks] 4 5 # our sort and groupby key function <ipython-input-9-2ec183945278> in <listcomp>(.0) 1 from collections import namedtuple 2 ----> 3 tracks = [fix_track(track) for track in tracks] 4 5 # our sort and groupby key function <ipython-input-5-9b75dc42f80b> in fix_track(track, fixer, fields, int_convert) 48 ''' 49 for f in fields: ---> 50 value = getattr(track, f) 51 if not value: 52 # value is likely None AttributeError: 'Track' object has no attribute 'title' Of course to avoid dealing with \"hanger on\" variables as I like to call them, this logic (as well as the logic to take a directory and extract it's subdirectories and finally the music files in it) could be stored in a function that'd be callable again and again. Something like this: def get_music_files ( basedir , valid_types = ( 'mp3' , 'ogg' , 'oga' , 'wav' , 'flac' ), ignore = None ): '''Walks the base directory to extract album directories from it. Then walks each album directory to extract valid audio files from it. The optional ignore attribute is a callback that allows you to dynamically ignore certain directories. ''' tracks = [] albums = [] for a in os . listdir ( basedir ): if ignore and ignore ( a ): continue albums . append ( path . join ( basedir , a )) for album in albums : _tracks = [] for t in os . listdir ( album ): if not t . endswith ( valid_types ): continue _tracks . append ( path . join ( album , t )) tracks . extend ( _tracks ) return tracks def convert_tinytags ( tracks , key_fields = ( 'artist' , 'album' , 'track' ), adaptor = None ): '''Accepts an iterable of TinyTag objects (or similar) and parses them into Artist, Album and Track objects. * tracks: iterable of track objects * key fields: fields used for sorting and grouping the track objects * adaptor: optional callback for extracting information from the old track objects to create the new track objects ''' key = namedtuple ( 'key' , key_fields ) def compare ( track ): values = [] for field in key_fields : values . append ( getattr ( track , field )) return key ( * values ) if not adaptor : adaptor = lambda t : { 'artist' : t . artist , 'album' : t . album , 'length' : t . duration , 'name' : t . title , 'track' : t . track } tracks . sort ( key = compare ) # this keeps us from creating a big list # with the same artist and albums over and over artists = set () albums = set () _tracks = set () for keys , grouped in it . groupby ( tracks , key = compare ): artists . add ( Artist ( name = keys . artist )) albums . add ( Album ( name = keys . album , artist = keys . artist )) for track in grouped : _tracks . add ( Track ( ** adaptor ( track ))) return list ( artists ), list ( albums ), list ( _tracks ) tracks = get_music_files ( '/home/justanr/Music/At the Drive‐In/' ) tracks = [ TinyTag . get ( t ) for t in tracks ] tracks = list ( map ( fix_track , tracks )) artists , albums , tracks = convert_tinytags ( tracks ) # of course you could just: # artists, albums, tracks = convert_tinytags([fix_track(TinyTag.get(t)) for t in get_music_files(...)]) # But that's also pretty hard to read, too print ( \"Artists: \" , * artists , sep = ' \\n ' ) print ( \" \\n Albums: \" , * sorted ( albums ), sep = ' \\n ' ) print ( \" \\n Tracks: \" , * sorted ( tracks ), sep = ' \\n ' ) Artists: <Artist name=At the Drive‐In id=2> Albums: <Album name=Relationship of Command artist=At the Drive‐In id=3> <Album name=Vaya artist=At the Drive‐In id=4> Tracks: <Track artist=At the Drive‐In album=Relationship of Command track=1 name=Arcarsenal id=27> <Track artist=At the Drive‐In album=Relationship of Command track=2 name=Pattern Against User id=28> <Track artist=At the Drive‐In album=Relationship of Command track=3 name=One Armed Scissor id=29> <Track artist=At the Drive‐In album=Relationship of Command track=4 name=Sleepwalk Capsules id=30> <Track artist=At the Drive‐In album=Relationship of Command track=5 name=Invalid Litter Dept. id=31> <Track artist=At the Drive‐In album=Relationship of Command track=6 name=Mannequin Republic id=32> <Track artist=At the Drive‐In album=Relationship of Command track=7 name=Enfilade id=33> <Track artist=At the Drive‐In album=Relationship of Command track=8 name=Rolodex Propaganda id=34> <Track artist=At the Drive‐In album=Relationship of Command track=9 name=Quarantined id=35> <Track artist=At the Drive‐In album=Relationship of Command track=10 name=Cosmonaut id=36> <Track artist=At the Drive‐In album=Relationship of Command track=11 name=Non‐Zero Possibility id=37> <Track artist=At the Drive‐In album=Vaya track=1 name=Rascuache id=38> <Track artist=At the Drive‐In album=Vaya track=2 name=Proxima Centauri id=39> <Track artist=At the Drive‐In album=Vaya track=3 name=Ursa Minor id=40> <Track artist=At the Drive‐In album=Vaya track=4 name=Heliotrope id=41> <Track artist=At the Drive‐In album=Vaya track=5 name=Metronome Arthritis id=42> <Track artist=At the Drive‐In album=Vaya track=6 name=300 MHz id=43> <Track artist=At the Drive‐In album=Vaya track=7 name=198d id=44> Of course, we could generalize get_music_files to recursively walk the base Music directory so it's not confined to my organizational scheme. def r_get_music_files ( basedir , valid_types = ( 'mp3' , 'ogg' , 'oga' , 'wav' , 'flac' ), ignore = None ): tracks = [] for f in os . listdir ( basedir ): f = path . join ( basedir , f ) # we can now filter both directories and files # passing the full path will even allow # ignore to filter based on if `f` is # a directory or file if ignore and ignore ( f ): continue elif path . isdir ( f ): tracks . extend ( r_get_music_files ( f , valid_types , ignore )) elif not f . endswith ( valid_types ): continue else : # must be a file we're looking for tracks . append ( f ) return tracks # only going to point this at a artist directory # the `An Awesome Wave` album is a directory in here # just to demonstrate that this does in fact work as intended tracks = r_get_music_files ( '/home/justanr/Music/Alt-J/' ) tracks = [ TinyTag . get ( t ) for t in tracks ] tracks = list ( map ( fix_track , tracks )) artists , albums , tracks = convert_tinytags ( tracks ) print ( \"Artists: \" , * artists , sep = ' \\n ' ) print ( \" \\n Albums: \" , * sorted ( albums ), sep = ' \\n ' ) print ( \" \\n Tracks: \" , * sorted ( tracks ), sep = ' \\n ' ) Artists: <Artist name=Alt-J id=3> Albums: <Album name=An Awesome Wave artist=Alt-J id=5> Tracks: <Track artist=Alt-J album=An Awesome Wave track=1 name=Intro id=45> <Track artist=Alt-J album=An Awesome Wave track=2 name=Intrelude 1 id=46> <Track artist=Alt-J album=An Awesome Wave track=3 name=Tesselate id=47> <Track artist=Alt-J album=An Awesome Wave track=4 name=Breezeblocks id=48> <Track artist=Alt-J album=An Awesome Wave track=5 name=Interlude 2 id=49> <Track artist=Alt-J album=An Awesome Wave track=6 name=Something Good id=50> <Track artist=Alt-J album=An Awesome Wave track=7 name=Disolve Me id=51> <Track artist=Alt-J album=An Awesome Wave track=8 name=Matilda id=52> <Track artist=Alt-J album=An Awesome Wave track=9 name=Ms id=53> <Track artist=Alt-J album=An Awesome Wave track=10 name=Fitzpleasure id=54> <Track artist=Alt-J album=An Awesome Wave track=11 name=Interlude 3 id=55> <Track artist=Alt-J album=An Awesome Wave track=12 name=Bloodflow id=56> <Track artist=Alt-J album=An Awesome Wave track=13 name=Taro id=57> That works, but it's recursive (in a bad way). And it stores everything in memory . Yuck. I have approximately 26,000 or so audio files chilling in my music directory. Do we really want to store all that information all at once when the ultimate goal is to simlply stuff it in the database (did I give away my plan?) and throw the immediate result away. You know what's really good at doing something and then throwing results away? Generators. Adapting r_get_music_files to a general file walker is trivial but I'm slightly vain and overly happy with my end generator. Before anyone jumps on me and says, \" ALEC ! There's os.walk! You've got os imported already!\" I know. os.walk almost does what we want: start with a folder, find the subfolders, yield a list of things. Except in this case it's three tuples: (dirpath, dirnames, filenames) . Which means we need to then parse that before getting to what we want. We could also use glob for this — iglob uses an iterator — but that delves into using regular expressions. There's pathlib in 3.4, but I'm ultimately unfamilar with it. def walk ( basedir , ignore = None ): for f in sorted ( os . listdir ( basedir )): # store fullpath separately so we can # easily pass it if needed fp = os . path . join ( basedir , f ) if ignore and ignore ( basedir , f ): continue elif os . path . isdir ( fp ): # to quote Dave Beazley: # \"Yield from is the ultimate not my problem. # It just says, 'Here's some generator, you deal with it.'\" yield from walk ( fp , ignore ) else : # must be a file we're looking for yield fp def ignore ( base , f , valid_types = ( '.mp3' , '.ogg' , '.oga' , '.wav' , '.flac' )): '''Example ignore function.''' fp = os . path . join ( base , f ) # ignore 'hidden' files and directories if f . startswith ( '.' ): return True # filter actual files based on their extension elif os . path . isfile ( fp ) and not f . lower () . endswith ( valid_types ): return True # got here, so we return False to *not* ignore this file # a little confusing # also, I don't like if/elif without a else else : return False # breaking the adaptor fully out into it's own function as well # even though for this example we're dealing exclusively with # TinyTag, I did give links to several other metadata libraries # maybe you're dealing with one of those? adaptor = lambda t : { 'artist' : t . artist , 'album' : t . album , 'length' : t . duration , 'name' : t . title , 'track' : t . track } def convert_track ( track , adaptor ): info = adaptor ( track ) artist = Artist ( name = info [ 'artist' ]) album = Album ( name = info [ 'album' ], artist = info [ 'artist' ]) track = Track ( ** info ) return artist , album , track % timeit - n 100 - r 5 next ( walk ( '/home/justanr/Music/' , ignore = ignore )) % timeit - n 100 - r 5 next ( os . walk ( '/home/justanr/Music' )) track = next ( walk ( '/home/justanr/Music/' , ignore = ignore )) track = TinyTag . get ( track ) track = fix_track ( track ) print ( * convert_track ( track , adaptor ), sep = ' \\n ' ) 100 loops, best of 5: 295 µs per loop 100 loops, best of 5: 1.34 ms per loop <Artist name=16 id=4> <Album name=Bridges to Burn artist=16 id=6> <Track artist=16 album=Bridges to Burn track=1 name=Throw in the Towel id=58> It's still procedural code but it's nicely composable. And it's quick. In this particular instance, it's faster than os.walk but I've always thought benchmarks are useless without a greater context — so for shits and giggles, I ran both over my Music directory (with all 26,000+ files), just spitting values into 0 length deque, and my walk function took about ~560ms, os.walk took ~350ms. Considering we're doing a little more work than os.walk, I'll take it any day of the week. And even though convert_track only accepts one track now, chunking over walk with islice is obvious. Now that I've digressed completely from audio metadata to quickly processing through files, I think I'll end here.","title":"Python and Audio Metadata"},{"tags":"tutorials","loc":"http://justanr.github.io/decorator-day","text":"An Aside: I originally wrote this as just a massive brain dump but in retrospect, it wasn't helpful at all. In the process of preparing a presentation for PyATL, I decided to rewrite it to be much more informative and less example heavy. The original post was essentially just a series of examples of how you'd use decorators or what you'd use them for rather than why you'd use decorators and addressing patterns. I'm hoping this rewrite clarifies that instead of being a \"Look at all this code you probably don't understand!\" Decorators are well championed but most posts I've seen about them don't address many of the issues here. I wouldn't say they're something beginner or novice friendly since it delves into concepts like functional programming and \"higher order functions\" which is just scary talk for \"functions that accept functions as arguments\" and even currying, which basically boils down to \"reducing the number of inputs a function takes.\" I'm grossly over simplifying here for the sake of explaination, but I stand by these simplifications. Another note, apparently there was a lot of hemming and hawing in the Python mailing list when this construct (for lack of a better word) was introduced because of confusion with the decorator design pattern. Given that design patterns aren't my strongest suit (at least in terms of knowing exactly what each does and how to best implement it), I think there's quite of bit of one-to-one between the two on a basic level. Each take in an object and modifies it in a way that the original shouldn't be worried about. The decorator pattern seems to be more concerned with adding data or processing information and is compositional whereas Python's decorators seem to be more about removing logic that, yes, is associated with the object in question but doesn't need to be mixed directly with the actual logic of the object. That makes it sound very confusing, but hopefully that will become clear through examples and explaination. Decorator Day This is meant to be a \"hit the ground running\" sort of thing decorators. Just enough information to make you feel confident but not enough that you know all there is to them. Though, honestly, once they click, they're not so complicated. I'm going to make the dangerous assumption that you have a decent grasp on functions and how Python handles them. I'm going to briefly cover a bit about that, but just what — I think — is requisite knowledge for understanding decorators. If you're confident with the concept of first class functions and closures, feel free to skip the next two sections. And just remember, a little knowledge is a dangerous thing; I'm giving you enough rope to hang yourself here but not enough to build a bridge. That comes with practice and exploration as well as digging deeper into subject matter. Don't mistake me for a guru expert code ninja wizard (though, I do hope to attain that title some day) as I'm in the mire with everyone else. First Class Functions The first, and probably most important, piece of background knowledge is that everything in Python is an object . Everything. Functions, Objects, Strings, and even Classes. This is what makes decorators possible in Python. We can assign functions to new names, stuff them in data structures, pass them as arguments to other functions and even return them from functions. That's really all there is to first class functions, they are treated just the same as anything else in the language. And since methods are simply functions attached to a class or object, the same applied to them as well. str.join is just as first class as map . my_func = dir funcs = [ filter , list , int ] map ( str , [ 1 , 2 , 3 ]) def returns_func (): return dict Closures The concept of closures scares a lot of people. But they're really simple: closures are just functions defined inside of another function. That's it. But there's magic involved, too. Closures close over the available outer scope and stuff it into their __closure__ attribute — which is just simply a tuple of cell objects that contain other things. This is essentially a snapshot of the outer function when it's created. Usually, the inner function is returned by the outer function. def adds_four (): x = 4 def inner ( y = 0 ): return x + y return inner o = adds_four () print ( o ()) print ( o . __closure__ ) 4 (<cell at 0x7f26c154bd68: int object at 0x9f8860>,) The same logic applies to values passed into a function as well. When an argument is passed into a function (say outer actually looked like outer(x=4) ), those values become part of the scope of the function as well and are available to closures for use if needed. Maybe I'll go deeper into closures in the future, maybe not. An aside If you're wondering how Python knows what cell object to use well, my understanding is that it doesn't and relies on the generated byte code to do the right thing. But I could very well be completely wrong on this matter. So, Decorators Now we finally get to decorators. Decorators are functions that take in a function, do something and return something — typically a callable, but it could be anything. Usually, decorators are used to either remove extraneous logic from a function or to attach additional functionality to it. There's two ways to use decorators: the old way, pre-2.5, and the special @decorator syntax. They do the same thing, like using list(1, 2, 3) or [1, 2, 3] , one's just syntatic sugar. def decorator ( f ): return f def my_func (): pass my_func = decorator ( my_func ) # this is the old way @decorator def my_func (): pass The @decorator syntax is the one you'll see in most cases. Basically, what you're telling Python is when it's done creating my_func pass it to decorator and then assign the result back to my_func . It sounds complicated to some, but it's really not. @decorator is just a way of shorthanding your intention to the interperter. And more importantly, it gives a heads up to people reading your code that something more is happening behind the scenes. If you manually wrap functions (which is needed sometimes), this can get lost at the bottom of your function definition unless you make glaringly obvious what's happening. This applies double to longer fuctions and objects. If you've got that down, you'll probably get the common patterns that follow. Patterns There's plenty of patterns you'll encounter when working with decorators. I'm hoping to identify some of the most common ones. Pass Through Decorators Basically, this is a decorator that takes in an object, does something and returns the original object. It might register it somewhere, gather some information about or add an attribute to it. def pass_through ( f ): f . inspected = True return f @pass_through def thing (): return \"Thing()\" assert hasattr ( thing , 'inspected' ) assert thing . __name__ == 'thing' It's common enough but usually they're not doing anything too exciting or surprising. But it's definitely worth distinguishing it from decorators that use closures. The rest of the patterns I'm going to cover are all function closure oriented, except for two which will be obvious. Flexible Decorators You're going to want your decorators to work with as many functions as possible. Not just the one you initially noticed and thought, \"I can factor this other code out into a decorator.\" But what does a flexible decorator look like? It's best to compare it to a brittle decorator to understand fully. def brittle ( f ): def wrapper ( arg1 , arg2 , kwarg1 = None , kwarg2 = None ): return f ( arg1 , arg2 , kwarg1 , kwarg2 ) return wrapper def flexible ( f ): def wrapper ( * args , ** kwargs ): return f ( * args , ** kwargs ) return wrapper What's so important about this that we end up using wrapper when we call a decorated function. If we used the first example, the brittle one, we'd have to write every function we decorate with it to accept four arguments. Where as the second example, the flexible one, can accept any number of positional or keyword arguments. To give a concrete example… @flexible def dinner ( food , beer ): # <- becomes wrapper print ( 'Eating' , food , 'and drinking' , beer ) dinner ( 'bratzel' , 'azazel' ) # <- actually wrapper Eating bratzel and drinking azazel I'm a little homesick for Memphis. If you're ever there, I'd recommend the Flying Saucer downtown. Anyways… Preserving metadata When you lock your original function up in a closure, you lose the original metadata: __name__ , __doc__ , etc. There's a few ways to retain it. Instead of hopping to the correct way, I'll show you why you'll want to do that instead. Just like ensuring defaults are set in a dictionary, there's an increasing level of \"correctness\" or \"Pythonness\" from what you should start with and what you should end with. The first thing you might try is manually transfering these attributes inside the decorator. def loggit ( f ): def logger ( * args , ** kwargs ): print ( \"Calling {}\" . format ( f . __name__ )) return f ( * args , ** kwargs ) logger . __name__ = f . __name__ logger . __doc__ = f . __doc__ logger . __dict__ . update ( f . __dict__ ) return logger Other than the fact that we're missing three crucial attributes, this is a terrible way to migrate attributes. You'll have to do this in every decorator you write — meaning it'll be no fun to maintain when a new attribute is introduced. This is very similiar to the basic idiom of checking if a key is in a dictionary before we do something with it: if something not in my_dict: my_dict[something] = [value] my_dict[something].append(value) Well, decorators are good at removing extraneous code from a function, let's try writing a decorator decorator — as confusing as that sounds, it's pretty common, so we're catching two pythons with one net. def decorator_decorator ( deco ): def wrapper ( f ): g = deco ( f ) g . __name__ = f . __name__ g . __doc__ = f . __doc__ g . __dict__ . update ( f . __dict__ ) return g wrapper . __name__ = deco . __name__ wrapper . __doc__ = deco . __doc__ wrapper . __dict__ . update ( deco . __dict__ ) return wrapper @decorator_decorator def loggit ( f ): def logger ( * args , ** kwargs ): print ( \"Calling {}\" . format ( f . __name__ )) return f ( * args , ** kwargs ) return logger @loggit def double ( x ): \"\"\"Doubles a number\"\"\" return 2 * x assert double . __name__ == 'double' assert double . __doc__ == 'Doubles a number' That's almost better. Except it's really ugly. Like really ugly. And actually, I find it more confusing than helpful. When I initially wrote this code it was as an example of what not to do and it's here for that same reason. It requires you to think on multiple levels at the same time, which is great if you're just trying to anger people looking over it. This decorator_decorator is pretty benign if you're comfortable with what each piece means and works. But if you're doing this in your code, unless you're having to work with 2.4 or below, you're doing it the wrong way. This is most akin to using dict.setdefault to ensure a value is present in a dictionary, and just like that method, I find it confusing every time I look at it (though, there are many times when it's useful): grouping = my_dict.setdefault(something, []) grouping.append(value) However, Python is batteries included and comes with something that handle this for us. Enter functools.wraps and functools.update_wrapper . @wraps is just the decorator form of update_wrapper — using a decorator to create decorators seems counter intuitive, but it's really not and @wraps is a testament to it's power and another example further down delves a little deeper. But update_wrapper is pretty simple, it merely: Copies needed attributes from the original function to the new This includes the name, docstring, module, qualified name and module attributes Updates the wrapper's dict with the original function's dict As of 3.2, it also supplies a __wrapped__ attribute that contains the original object It doesn't appear that this was backported to Python 2 at all. I'd seriously recommend monkey patching update_wrapper to handle this for reasons that'll become obvious later. update_wrapper is also smart enough to not blow up if one of these attributes isn't present on the underlying object (such as __name__ on an instance of an object). Here's how you should be perserving metadata through decorators: from functools import wraps def loggit ( f ): @wraps ( f ) def logger ( * args , ** kwargs ): print ( \"Calling {}\" . format ( f . __name__ )) return f ( * args , ** kwargs ) return logger @loggit def double ( x ): \"\"\"Doubles a number\"\"\" return 2 * x assert double . __name__ == 'double' assert double . __doc__ == 'Doubles a number' assert not double . __annotations__ Using @wraps and update_wrapper is like when you stumble across defaultdict in the collections module. The complexity and hand holding you were previously doing just melts away into a few easy to grasp lines of code: my_dict = defaultdict(list) my_dict[something].append(value) Minimum Decorator Best Practices The two best things you can do for yourself is writing flexible decorators (using *args and **kwargs ) and preserving metadata through wraps and update_wrapper . 80% of the headaches you'll encounter will be fixed with these. The other 20% are…tricky. I'll touch on those a little later, however. Decorators that accept optional kwargs Occasionally, you need to use keyword arguments in your decorators, either to pass context, run setup or just set flags. There's a few ways to do it… def print_before ( phrase = \"Setting up!\" ): def actual_decorator ( f ): @wraps ( f ) def wrapper ( * args , ** kwargs ): print ( phrase ) return f ( * args , ** kwargs ) return wrapper return actual_decorator @print_before ( phrase = \"such setup\" ) def doubles ( x ): return 2 * x @print_before () # parens required def random (): return 4 In case you're wondering, the parens are required because we need to call the outermost function, which generates the decorator which in turn generators the wrapper function we're actually using. To be 100% honest, outside of a handful of cases, I find this particular implementation to be lackluster because usually the outermost function just serves to pass information one level deeper. However, one of the best arguments for using this particular pattern is passing positional arguments to a decorator. This has more to do with how Python handles unpacking into functions than anything else. But using an outer function like this is certainly the easiest way I've thought of to handle positional arguments to a decorator. You could fiddle with pushing the fuction to the front of *args , but I feel it would quickly become messy. Back to optional kwargs, instead of having an outermost function that we must explictly call, why not use functools.partial and setup a little check? from functools import partial def print_before ( f = None , * , phrase = \"Settin up!\" ): if f is None : return partial ( print_before , phrase = phrase ) @wraps ( f ) def wrapper ( * args , ** kwargs ): print ( phrase ) return f ( * args , ** kwargs ) return wrapper @print_before # parens not needed def double ( x ): return 2 * x @print_before ( phrase = \"such dice roll!\" ) def random (): return 4 That's actually a lot better. The most interesting thing here is how using partial allows us to for go the parens when we don't overide keyword arguments. Since we're accepting only one positional argument (via Python 3's keyword only syntax, there's not really a good way to imitate this is Python 2 other than using positional arguments sparingly when calling a function), when Python puts the function into the decorator, it does it positionally. We don't need to call the outermost function explicitly because Python will call it for us. And if we need to override a keyword argument, we can call the function explicitly to do so. However, we've just moved our primary concern inside the decorator and it's creating noise. Moreover, if we decide that this is the best thing in the world, we'll be copying that code into every decorator we write. And six months from now, when partial is considered as terrible as GOTO , we'll have to replace all these instances (though, you can never take my partials from me!). I'm not sure if that's better or worse than before. We're already writing a decorator, and we're getting pretty good at them, so why not a decorator that'll handle this for us? This is probably the most confusing example, so hold on: def optional_kwargs ( deco ): @wraps ( deco ) def wrapper ( f = None , ** kwargs ): # <- actual function our decorator becomes if f is None : return partial ( wrapper , ** kwargs ) return deco ( f , ** kwargs ) return wrapper @optional_kwargs def print_before ( f , * , phrase = \"Setting up!\" ): # <- becomes the closure inside in optional_kwargs @wraps ( f ) def wrapper ( * args , ** kwargs ): print ( phrase ) return f ( * args , ** kwargs ) return wrapper @print_before ( phrase = \"much mind bend!\" ) # <- optional_kwarg's closure def random (): # <- becomes print_before's closure return 4 This looks really complex and intimidating, but it's really not when you get to understand decorators. All we've done is factor out the code print_before shouldn't be concerned with, namely if it actually recieved a function or not. What happens is that we replace print_before with the closure inside optional_kwargs . This closure handles the logic of ensuring that a function was passed or not. The true power of this becomes apparent when you replace the print statement with something that will preprocess inputs for you: escaping characters or changing integers and floats to Decimal if you're working with financial data. Here, it's simply to illustrate how you would implement the pattern. Like the horrible decorator_decorator example above, this sort of code forces your brain to operate on multiple levels at the same time. Once you're comfortable with it, this makes quite a bit of sense as to why you'd do it. However, before that, it merely causes a headache. Using classes as decorators Sometimes a function closure doesn't provide the umph needed to do something, or packing in the umph creates a twisted web of logic that causes your eyes to roll into the back of your head when you revisit it. Sometimes you just need a good ‘ol object to handle things for you. There's two different ways to use objects as decorators, one is to use one instance per wrap and the other is to use one instance for every wrap. from functools import update_wrapper class wrapperobj : def __init__ ( self , f ): self . f = f update_wrapper ( self , f ) def __call__ ( self , * args , ** kwargs ): return self . f ( * args , ** kwargs ) @wrapperobj def random (): return 4 When using a single instance per wrap, you need to first store the wrapped obj on the instance ( self.f ) and then when you need to call it, you simply pass it arguments. Fairly straight forward, but it's pretty contrived example as well. This sort of pattern would be used when your logic becomes much more complex than what a simple function should reasonably handle, things like transcieving data to an event loop or socket for example or creating individual caches for each object. Let's look at using a single instance to wrap multiple objects. class onewrapper : def __call__ ( self , f ): @wraps ( f ) def wrapper ( * args , ** kwargs ): return f ( * args , ** kwargs ) return wrapper inst = onewrapper () @inst def random (): return 4 This is essentially the same thing as using a function closure as a decorator, except it comes with the extra umph that comes with using an object as well. This sort of pattern would be used for linking several objects together in a shared state, a rudimentary messaging queue should be easy to write with this sort of pattern. Your __init__ method in this case would set the initial state for the object rather than accepting the function there. And of course, you don't have to use __init__ or __call__ to register and run functions, you can create other methods that would handle these as well. register and run_wrapped for example could easily do the same thing. If you're wrapping multiple objects with the same instance, it may be helpful to also use some sort of data structure to organize them if you're doing more than simply wrapping them. In most cases the weakref module will either provide the answer you need or set you on the right path; though, there are objects that can't be weak referenced — including most of the built in or \"primitive\" types in Python (str, int, float, list, dict and tuple). However, for more complex objects, weakrefs are a powerful tool that let you conserve memory among other things. It's worth balancing using something like WeakKeyDictionary versus a regular dictionary for caching. I got sidelined, anyways… Decorating Classes As of Python 2.6, decorating classes is also supported. There's all the same reasons you'd want to decorate a function with the added benefit that you can apply metaclasses uniformly between Python 2 and 3. Or maybe it's just simpler than writing a metaclass in the first place. This next example comes from the six.py project and allows you to decorate a class to be applied in Python 2 and 3. # from six.py def add_metaclass ( metaclass ): \"\"\"Class decorator for creating a class with a metaclass.\"\"\" def wrapper ( cls ): orig_vars = cls . __dict__ . copy () slots = orig_vars . get ( '__slots__' ) if slots is not None : if isinstance ( slots , str ): slots = [ slots ] for slots_var in slots : orig_vars . pop ( slots_var ) orig_vars . pop ( '__dict__' , None ) orig_vars . pop ( '__weakref__' , None ) return metaclass ( cls . __name__ , cls . __bases__ , orig_vars ) return wrapper Applying multiple decorators This generally works. Especially if you've used wraps or update_wrapper consistently. The important thing to note here is that decorators are evaluated from the bottom up. The first thing that happens is that Python defines the original object, then it walks up each decorator, passing the return value from each to the next decorator. That's pretty much all there is to stacking decorators, except for one thing that I'll cover in the next section. @wrapperobj @print_before def random (): return 4 random () Setting up! 4 One nice thing is if you remember to use wraps or update_wrapper , Python will very helpfully pass along added attributes as well (since they are really just key-value pairs sitting in __dict__ ). In case you forgot, pass_through is the first example in this section, it merely adds an attribute called inspected to an object. @wrapperobj @pass_through def random (): return 4 assert hasattr ( random , 'inspected' ) print ( random . __dict__ ) {'__annotations__': {}, 'f': <function random at 0x7f26c0039d08>, '__qualname__': 'random', '__wrapped__': <function random at 0x7f26c0039d08>, 'inspected': True, '__doc__': None, '__module__': '__main__', '__name__': 'random'} Issues and Problems As with anything, there are certain problems and issues you'll run into. Decorators are no exception. At some point, at least one of these will bite you, maybe hard. Stacking wrong This is something that will happen more often then you think. However, as long as you remember that they're evaluated from the bottom up and you've used wraps and update_wrapper through out, you'll probably be okay. Sometimes, it's important to note which decorators allow an object to pass through and which lock the object in a closure — you shouldn't have to, but at least once you will. However, there is something to be said for the @property , @classmethod and @staticmethod decorators. These return *descriptors* which are a slightly different sort of object that behave in a peculiar way. I've gone into detail about them else where, but the rule of thumb is to always apply these decorators last. Testing This is something of a sticky point. Pass through decorators are easy to test, those wrapped objects aren't hidden away anywhere. However, you're far more likely to encounter a closure decorator which makes testing much harder. The easiest way is to use the __wrapped__ attribute if it's available. Python 3.4 does provide a very nice function in inspect called unwrap which continually unravels a decorated function until it's no longer wrapped up. But if neither of these are available to you, and you're decorator does something like connect to a database or ensure a user's logged in, you might be in for a head ache. Introspection and blowing stuff up. More than once I've been bitten by bad introspection (looking at you, Google API wrapper), thankfully it was only me manually inspecting a function to see how to use it. Even using update_wrapper isn't enough to solve this problem. Argument specification Closely related is annotations in Python 3 Ability to pull source code type and isinstance stop working I'll admit blowing up code is one of the ways I learn the best. \"What did I do that I shouldn't have done and how do I fix it?\" Though, sometimes the errors are hard to track down. Python 2's inspect.getargspec doesn't like being passed an object and complains loudly about it. Python 3's version doesn't seem to mind, though take that with a grain of salt because I'm sure there's a corner case somewhere that blows it up. Python 2 will display the argument specification and source code of the wrapper. Python 3 seems to do the right thing (at least in 3.4, and this may be because of inspect.unwrap and the __wrapped__ attribute). Wrapping classes can seriously put a kink in your inheritance pattern if you're not careful about it. If your class simply passes through the decorator, you'll be okay. If it doesn't, you need to take care to make sure to either return the actual class (most likely using descriptors) or use something like __wrapped__ or inspect.unwrap in your inheritance line, both are which are ugly. Similar to inheritance, isinstance isn't too happy about having a function passed to it. This behavior looks consistent between Python 2 and 3. The same fixes should apply here. The final thing, which I'm unsure about fixing, is using type to identify explicitly what an object is. Descriptors seems like they should work and rudimentary testing with it seems to cause type to behave as expected, but I've not battle tested this — and the real question is, \"Why are you using type anyways?\" Actually fixing these If you're much more curious about the proper way to fix these issues, I highly recommend Graham Dumpleton's blog series How you implemented your Python decorator is wrong. which goes into so much more depth than I have here and really opened my eyes to what I was actually doing when writing decorators beyond just stuffing one function inside of another. It also serves as a decent introduction into how descriptors work and practical uses of them. No Man is an Island Unto Himself This post wouldn't have been possible without the writings of Graham Dumpleton (sourced above), Jeff Knupp and Simeon Franklin . All three of these — as well as countless other blog posts and StackOverflow questions — introduced me and explained what decorators are to me. I'd also like to give thanks to both PyATL and Doug Hellman for sparking my interest in giving a presentation as well as the online learning group I'm a member of for sitting through about forty minutes of me listening to the sound of my own voice and asking for clarifications on many things I glossed over initially, thinking that people think the same way I do (which is probably a lot more twisted and convoluted than most). I'm probably going to rewrite several other posts as well in a more frank \"This is what's happening\" manner instead of the usual \"Here's a bunch of code, I hope you get it!\" that makes sense to me more than anyone else.","title":"Decorator Day"}]}